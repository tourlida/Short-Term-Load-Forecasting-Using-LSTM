{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seasonal_comparison(test_index, ys_test_rescaled, predictions_rescaled):\n",
    "    \"\"\"\n",
    "    Plot the average actual and forecasted load for each season.\n",
    "\n",
    "    Parameters:\n",
    "    - test_index: Datetime index for the test data.\n",
    "    - ys_test_rescaled: Rescaled actual test values.\n",
    "    - predictions_rescaled: Rescaled forecasted values.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame to hold the test and prediction data\n",
    "    data = pd.DataFrame({\n",
    "        'Actual': ys_test_rescaled.flatten(),\n",
    "        'Predicted': predictions_rescaled.flatten()\n",
    "    }, index=test_index)\n",
    "    \n",
    "    # Add a 'Season' column to the DataFrame\n",
    "    data['Month'] = data.index.month\n",
    "    data['Season'] = data['Month'].apply(lambda x: (\n",
    "        'Winter' if x in [12, 1, 2] else\n",
    "        'Spring' if x in [3, 4, 5] else\n",
    "        'Summer' if x in [6, 7, 8] else\n",
    "        'Autumn'\n",
    "    ))\n",
    "    \n",
    "    # Group by season and calculate the mean for actual and predicted values\n",
    "    seasonal_data = data.groupby('Season').mean()\n",
    "    \n",
    "    # Plot the seasonal comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(seasonal_data.index, seasonal_data['Actual'], label='Actual Load', marker='o')\n",
    "    plt.plot(seasonal_data.index, seasonal_data['Predicted'], label='Forecasted Load', marker='o')\n",
    "    plt.xlabel('Season')\n",
    "    plt.ylabel('Average Load')\n",
    "    plt.title('Average Actual Load and Forecasted Load by Season')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_monthly_comparison(test_index, ys_test_rescaled, predictions_rescaled):\n",
    "    \"\"\"\n",
    "    Plot the average actual and forecasted load by month.\n",
    "\n",
    "    Parameters:\n",
    "    - test_index: Datetime index for the test data.\n",
    "    - ys_test_rescaled: Rescaled actual test values.\n",
    "    - predictions_rescaled: Rescaled forecasted values.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame to hold the test and prediction data\n",
    "    data = pd.DataFrame({\n",
    "        'Actual': ys_test_rescaled.flatten(),\n",
    "        'Predicted': predictions_rescaled.flatten()\n",
    "    }, index=test_index)\n",
    "    \n",
    "    # Add a 'Month' column to the DataFrame\n",
    "    data['Month'] = data.index.month\n",
    "\n",
    "    # Group by month and calculate the mean for actual and predicted values\n",
    "    monthly_data = data.groupby('Month').mean()\n",
    "\n",
    "    # Create month labels corresponding to the months present in the dataset\n",
    "    month_labels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    available_months = monthly_data.index\n",
    "    available_labels = [month_labels[month - 1] for month in available_months]\n",
    "\n",
    "    # Plot the monthly comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(monthly_data.index, monthly_data['Actual'], label='Actual Load', marker='o')\n",
    "    plt.plot(monthly_data.index, monthly_data['Predicted'], label='Forecasted Load', marker='o')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Average Load')\n",
    "    plt.title('Average Actual Load and Forecasted Load by Month')\n",
    "    plt.xticks(ticks=monthly_data.index, labels=available_labels)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weekday_comparison(test_index, ys_test_rescaled, predictions_rescaled):\n",
    "    \"\"\"\n",
    "    Plot the average actual and forecasted load by day of the week.\n",
    "\n",
    "    Parameters:\n",
    "    - test_index: Datetime index for the test data.\n",
    "    - ys_test_rescaled: Rescaled actual test values.\n",
    "    - predictions_rescaled: Rescaled forecasted values.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame to hold the test and prediction data\n",
    "    data = pd.DataFrame({\n",
    "        'Actual': ys_test_rescaled.flatten(),\n",
    "        'Predicted': predictions_rescaled.flatten()\n",
    "    }, index=test_index)\n",
    "    \n",
    "    # Add a 'DayOfWeek' column to the DataFrame\n",
    "    data['DayOfWeek'] = data.index.dayofweek\n",
    "\n",
    "    # Group by day of the week and calculate the mean for actual and predicted values\n",
    "    weekday_data = data.groupby('DayOfWeek').mean()\n",
    "\n",
    "    # Create day labels corresponding to the days of the week\n",
    "    day_labels = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    available_days = weekday_data.index\n",
    "    available_day_labels = [day_labels[day] for day in available_days]\n",
    "\n",
    "    # Plot the weekday comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(weekday_data.index, weekday_data['Actual'], label='Actual Load', marker='o')\n",
    "    plt.plot(weekday_data.index, weekday_data['Predicted'], label='Forecasted Load', marker='o')\n",
    "    plt.xlabel('Day of the Week')\n",
    "    plt.ylabel('Average Load')\n",
    "    plt.title('Average Actual Load and Forecasted Load by Day of the Week')\n",
    "    plt.xticks(ticks=weekday_data.index, labels=available_day_labels)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hourly_comparison(test_index, ys_test_rescaled, predictions_rescaled):\n",
    "    \"\"\"\n",
    "    Plot the average actual and forecasted load by hour of the day.\n",
    "\n",
    "    Parameters:\n",
    "    - test_index: Datetime index for the test data.\n",
    "    - ys_test_rescaled: Rescaled actual test values.\n",
    "    - predictions_rescaled: Rescaled forecasted values.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame to hold the test and prediction data\n",
    "    data = pd.DataFrame({\n",
    "        'Actual': ys_test_rescaled.flatten(),\n",
    "        'Predicted': predictions_rescaled.flatten()\n",
    "    }, index=test_index)\n",
    "    \n",
    "    # Add an 'Hour' column to the DataFrame\n",
    "    data['Hour'] = data.index.hour\n",
    "\n",
    "    # Group by hour of the day and calculate the mean for actual and predicted values\n",
    "    hourly_data = data.groupby('Hour').mean()\n",
    "\n",
    "    # Create hour labels corresponding to the hours of the day\n",
    "    available_hours = hourly_data.index\n",
    "    available_hour_labels = [f'{hour}:00' for hour in available_hours]\n",
    "\n",
    "    # Plot the hourly comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(hourly_data.index, hourly_data['Actual'], label='Actual Load', marker='o')\n",
    "    plt.plot(hourly_data.index, hourly_data['Predicted'], label='Forecasted Load', marker='o')\n",
    "    plt.xlabel('Hour of the Day')\n",
    "    plt.ylabel('Average Load')\n",
    "    plt.title('Average Actual Load and Forecasted Load by Hour of the Day')\n",
    "    plt.xticks(ticks=hourly_data.index, labels=available_hour_labels)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_from_to(test_index, ys_test_rescaled, predictions_rescaled, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Plot the actual and forecasted load for a specified date range.\n",
    "\n",
    "    Parameters:\n",
    "    - test_index: Datetime index for the test data.\n",
    "    - ys_test_rescaled: Rescaled actual test values.\n",
    "    - predictions_rescaled: Rescaled forecasted values.\n",
    "    - start_date: Start date for the plot.\n",
    "    - end_date: End date for the plot.\n",
    "    \"\"\"\n",
    "    # Convert start_date and end_date to datetime if they are strings\n",
    "    if isinstance(start_date, str):\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "    if isinstance(end_date, str):\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    # Create a boolean mask for the date range\n",
    "    mask = (test_index >= start_date) & (test_index <= end_date)\n",
    "\n",
    "    # Apply the mask to the test data and predictions\n",
    "    time_index = test_index[mask]\n",
    "    ys_test_range = ys_test_rescaled[mask]\n",
    "    predictions_range = predictions_rescaled[mask]\n",
    "\n",
    "    # Plotting the actual and forecasted load for the specified date range\n",
    "    plt.figure(figsize=(24, 5))\n",
    "    plt.plot(time_index, ys_test_range.flatten(), label='Actual Load')\n",
    "    plt.plot(time_index, predictions_range.flatten(), label='Forecasted Load')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Load')\n",
    "    plt.title(f'Actual Load and Forecasted Load from {start_date:%Y-%m-%d} to {end_date:%Y-%m-%d}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Customize x-axis to show date and day of the week\n",
    "    plt.xticks(ticks=time_index[::24], labels=[f\"{date:%Y-%m-%d}\\n{date:%A}\" for date in time_index[::24]], rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_results(test_index, ys_test_rescaled, predictions_rescaled, hours_to_plot=720):\n",
    "    \"\"\"\n",
    "    Plot the actual and forecasted load for the first month.\n",
    "\n",
    "    Parameters:\n",
    "    - test_index: Datetime index for the test data.\n",
    "    - ys_test_rescaled: Rescaled actual test values.\n",
    "    - predictions_rescaled: Rescaled forecasted values.\n",
    "    - hours_in_month: Number of hours to plot for the first month.\n",
    "    \"\"\"\n",
    "    # Adjust the test index to include only the first month of data\n",
    "    time_index = test_index[:hours_to_plot]\n",
    "    ys_test_first_month = ys_test_rescaled[:hours_to_plot]\n",
    "    predictions_first_month = predictions_rescaled[:hours_to_plot]\n",
    "\n",
    "    # Plotting the initial month of actual and forecasted load\n",
    "    plt.figure(figsize=(24, 5))\n",
    "    plt.plot(time_index, ys_test_first_month.flatten(), label='Actual Load')\n",
    "    plt.plot(time_index, predictions_first_month.flatten(), label='Forecasted Load')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Load')\n",
    "    plt.title('Actual Load and Forecasted Load for the First Month')\n",
    "    plt.legend()\n",
    "\n",
    "    # Customize x-axis to show date and day of the week\n",
    "    plt.xticks(ticks=time_index[::24], labels=[f\"{date:%Y-%m-%d}\\n{date:%A}\" for date in time_index[::24]], rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_model(xs_train, ys_train, model_config, num_target_features,path_to_save_model):\n",
    "    \"\"\"\n",
    "    Build, train, and evaluate an LSTM model.\n",
    "\n",
    "    Parameters:\n",
    "    - xs_train, ys_train: Training data.\n",
    "    - xs_test, ys_test: Test data.\n",
    "    - model_config: Dictionary containing LSTM layers configuration and other model parameters.\n",
    "    - scaler: Scaler used to scale data.\n",
    "    - num_target_features: Number of output features for the model.\n",
    "\n",
    "    Returns:\n",
    "    - model: Trained LSTM model.\n",
    "    - history: Training history of the model.\n",
    "    - predictions_rescaled: Rescaled predictions.\n",
    "    - ys_test_rescaled: Rescaled actual values.\n",
    "    \"\"\"\n",
    "    # Build the LSTM model\n",
    "\n",
    "    # Determine the split index\n",
    "    split_index = int(len(xs_train) * 0.8)  # 80% for training, 20% for validation\n",
    "\n",
    "    # Split the data sequentially\n",
    "    xs_train_split = xs_train[:split_index]\n",
    "    ys_train_split = ys_train[:split_index]\n",
    "    xs_val_split = xs_train[split_index:]\n",
    "    ys_val_split = ys_train[split_index:]\n",
    "    num_samples =  xs_train.shape[0]\n",
    "    num_time_steps = xs_train.shape[1]\n",
    "    num_features = xs_train.shape[2]\n",
    "    input_shape = (num_time_steps, num_features)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    for layer_config in model_config['cnn_layers']:\n",
    "        model.add(Conv1D(filters=layer_config.get('filters', 32), kernel_size=layer_config.get('kernel_size', 5), activation='relu', input_shape=input_shape))\n",
    "        model.add(MaxPooling1D(pool_size=layer_config.get('pool_size', 2)))\n",
    "         # Reset input_shape after first layer\n",
    "        input_shape = None\n",
    "        \n",
    "        if 'dropout' in layer_config:\n",
    "            model.add(Dropout(layer_config['dropout']))\n",
    "            \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_target_features))\n",
    "        \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    # Define the early stopping and model checkpoint callbacks\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "    checkpoint = ModelCheckpoint(path_to_save_model, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    history = model.fit(xs_train_split, ys_train_split, epochs=model_config.get('epochs', 50), batch_size=model_config.get('batch_size', 32), validation_data=(xs_val_split, ys_val_split), callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length, forecast_horizon, target_col):\n",
    "    target_col_index = target_col\n",
    "    xs, ys = [], []\n",
    "    target_col_name = data.columns[target_col_index]  \n",
    "    for i in range(len(data) - seq_length - forecast_horizon + 1):\n",
    "        x = data.iloc[i:(i + seq_length)].values\n",
    "      #  x = data.iloc[i:(i + seq_length)].drop(columns=[target_col_name]).values \n",
    "        y = data.iloc[(i + seq_length):(i + seq_length + forecast_horizon), target_col].values\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "#datapath= '../data/processed/covid-data/covid_dataset_without_outliers.csv'\n",
    "train_data_df = pd.read_csv('../data/processed/covid-data/train/covid_dataset_actuals_train.csv', parse_dates=['Time'], index_col='Time')\n",
    "# Corrected test data: \n",
    "test_data_df = pd.read_csv('../data/processed/covid-data/test/covid_dataset_corrected_weather_forecasts.csv', parse_dates=['Time'], index_col='Time')\n",
    "#Forecasts: \n",
    "#test_data_df = pd.read_csv('../data/processed/covid-data/test/covid_dataset_forecasts_test.csv', parse_dates=['Time'], index_col='Time')\n",
    "#Actuals:\n",
    "#test_data_df = pd.read_csv('../data/processed/covid-data/test/covid_dataset_actuals_test.csv', parse_dates=['Time'], index_col='Time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit number of data to have same size as evalution period\n",
    "EVALUATION_PERIOD_LENGTH = 744\n",
    "test_data_df = test_data_df.iloc[:EVALUATION_PERIOD_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature (C)</th>\n",
       "      <th>Wind Direction (deg)</th>\n",
       "      <th>Wind Speed (kmh)</th>\n",
       "      <th>hour_of_day_sin</th>\n",
       "      <th>hour_of_day_cos</th>\n",
       "      <th>Load (kW)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-03-18 00:00:00</th>\n",
       "      <td>16.764386</td>\n",
       "      <td>285.25</td>\n",
       "      <td>6.25375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.031472e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-18 01:00:00</th>\n",
       "      <td>15.729386</td>\n",
       "      <td>285.00</td>\n",
       "      <td>9.20500</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>1.007206e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-18 02:00:00</th>\n",
       "      <td>15.816886</td>\n",
       "      <td>316.00</td>\n",
       "      <td>11.28375</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>9.861084e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-18 03:00:00</th>\n",
       "      <td>15.919386</td>\n",
       "      <td>327.00</td>\n",
       "      <td>7.16875</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>9.707610e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-18 04:00:00</th>\n",
       "      <td>15.769386</td>\n",
       "      <td>282.25</td>\n",
       "      <td>0.78000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>9.622584e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Temperature (C)  Wind Direction (deg)  Wind Speed (kmh)  \\\n",
       "Time                                                                           \n",
       "2017-03-18 00:00:00        16.764386                285.25           6.25375   \n",
       "2017-03-18 01:00:00        15.729386                285.00           9.20500   \n",
       "2017-03-18 02:00:00        15.816886                316.00          11.28375   \n",
       "2017-03-18 03:00:00        15.919386                327.00           7.16875   \n",
       "2017-03-18 04:00:00        15.769386                282.25           0.78000   \n",
       "\n",
       "                     hour_of_day_sin  hour_of_day_cos     Load (kW)  \n",
       "Time                                                                 \n",
       "2017-03-18 00:00:00         0.000000         1.000000  1.031472e+06  \n",
       "2017-03-18 01:00:00         0.258819         0.965926  1.007206e+06  \n",
       "2017-03-18 02:00:00         0.500000         0.866025  9.861084e+05  \n",
       "2017-03-18 03:00:00         0.707107         0.707107  9.707610e+05  \n",
       "2017-03-18 04:00:00         0.866025         0.500000  9.622584e+05  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature (C)</th>\n",
       "      <th>Wind Direction (deg)</th>\n",
       "      <th>Wind Speed (kmh)</th>\n",
       "      <th>hour_of_day_sin</th>\n",
       "      <th>hour_of_day_cos</th>\n",
       "      <th>Load (kW)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-06-27 04:00:00</th>\n",
       "      <td>15.242884</td>\n",
       "      <td>191.875</td>\n",
       "      <td>12.7350</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>9.705132e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-27 05:00:00</th>\n",
       "      <td>14.794327</td>\n",
       "      <td>198.750</td>\n",
       "      <td>10.2550</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>2.588190e-01</td>\n",
       "      <td>9.694804e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-27 06:00:00</th>\n",
       "      <td>14.929302</td>\n",
       "      <td>212.750</td>\n",
       "      <td>14.2875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>9.644836e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-27 07:00:00</th>\n",
       "      <td>15.765862</td>\n",
       "      <td>217.250</td>\n",
       "      <td>14.2325</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>-2.588190e-01</td>\n",
       "      <td>9.810288e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-27 08:00:00</th>\n",
       "      <td>18.017364</td>\n",
       "      <td>207.250</td>\n",
       "      <td>15.4800</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>1.021520e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Temperature (C)  Wind Direction (deg)  Wind Speed (kmh)  \\\n",
       "Time                                                                           \n",
       "2020-06-27 04:00:00        15.242884               191.875           12.7350   \n",
       "2020-06-27 05:00:00        14.794327               198.750           10.2550   \n",
       "2020-06-27 06:00:00        14.929302               212.750           14.2875   \n",
       "2020-06-27 07:00:00        15.765862               217.250           14.2325   \n",
       "2020-06-27 08:00:00        18.017364               207.250           15.4800   \n",
       "\n",
       "                     hour_of_day_sin  hour_of_day_cos     Load (kW)  \n",
       "Time                                                                 \n",
       "2020-06-27 04:00:00         0.866025     5.000000e-01  9.705132e+05  \n",
       "2020-06-27 05:00:00         0.965926     2.588190e-01  9.694804e+05  \n",
       "2020-06-27 06:00:00         1.000000     6.123234e-17  9.644836e+05  \n",
       "2020-06-27 07:00:00         0.965926    -2.588190e-01  9.810288e+05  \n",
       "2020-06-27 08:00:00         0.866025    -5.000000e-01  1.021520e+06  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_col = train_data_df.pop('Load (kW)')\n",
    "train_data_df['Load (kW)'] = load_col\n",
    "\n",
    "target_col = (train_data_df.columns.get_loc('Load (kW)'))\n",
    "num_target_features = 1\n",
    "scaler_num_features = train_data_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Scale data\n",
    "scaler = MinMaxScaler()\n",
    "train_data_df_scaled = pd.DataFrame(scaler.fit_transform(train_data_df), columns=train_data_df.columns, index=train_data_df.index)\n",
    "test_data_df_scaled = pd.DataFrame(scaler.fit_transform(test_data_df), columns=test_data_df.columns, index=test_data_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Temperature (C)', 'Wind Direction (deg)', 'Wind Speed (kmh)',\n",
       "       'hour_of_day_sin', 'hour_of_day_cos', 'Load (kW)'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scaler to a file using joblib\n",
    "#dump(scaler, '../models/scalers/scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28716, 4, 6) (28716, 1)\n",
      "(740, 4, 6) (740, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#2. Create sequences pairs of input and output\n",
    "#In this case we have to configure the target_col-1 to be the index of the target column in the data_df in order to assign in the ys variable\n",
    "# and have input output pairs of sequences\n",
    "# seq_length = 8\n",
    "seq_length = 4\n",
    "forecast_horizon = 1\n",
    "xs_train_scaled, ys_train_scaled = create_sequences(train_data_df_scaled, seq_length, forecast_horizon, target_col)\n",
    "xs_test_scaled, ys_test_scaled = create_sequences(test_data_df_scaled, seq_length, forecast_horizon, target_col)\n",
    "\n",
    "print(xs_train_scaled.shape, ys_train_scaled.shape)\n",
    "print(xs_test_scaled.shape, ys_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x3301f7fb0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 4))\n",
    "train_data_df['Load (kW)'].plot(ax=ax, label=\"Train\")\n",
    "test_data_df['Load (kW)'].plot(ax=ax, label=\"Test\", linestyle='dotted')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.49311621, 0.79818118, 0.1896225 , 0.5       , 1.        ,\n",
       "         0.24540652],\n",
       "        [0.46247224, 0.79748164, 0.27910855, 0.62940952, 0.98296291,\n",
       "         0.21339019],\n",
       "        [0.46506292, 0.88422525, 0.34213918, 0.75      , 0.9330127 ,\n",
       "         0.18555483],\n",
       "        [0.46809771, 0.91500525, 0.21736659, 0.85355339, 0.85355339,\n",
       "         0.16530561]],\n",
       "\n",
       "       [[0.46247224, 0.79748164, 0.27910855, 0.62940952, 0.98296291,\n",
       "         0.21339019],\n",
       "        [0.46506292, 0.88422525, 0.34213918, 0.75      , 0.9330127 ,\n",
       "         0.18555483],\n",
       "        [0.46809771, 0.91500525, 0.21736659, 0.85355339, 0.85355339,\n",
       "         0.16530561],\n",
       "        [0.46365655, 0.78978664, 0.0236507 , 0.9330127 , 0.75      ,\n",
       "         0.15408738]],\n",
       "\n",
       "       [[0.46506292, 0.88422525, 0.34213918, 0.75      , 0.9330127 ,\n",
       "         0.18555483],\n",
       "        [0.46809771, 0.91500525, 0.21736659, 0.85355339, 0.85355339,\n",
       "         0.16530561],\n",
       "        [0.46365655, 0.78978664, 0.0236507 , 0.9330127 , 0.75      ,\n",
       "         0.15408738],\n",
       "        [0.46195411, 0.45540399, 0.01368253, 0.98296291, 0.62940952,\n",
       "         0.15618463]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.58675056, 0.60860441, 0.52198302, 0.85355339, 0.14644661,\n",
       "         0.39937263],\n",
       "        [0.62057735, 0.59706191, 0.62738781, 0.75      , 0.0669873 ,\n",
       "         0.45063804],\n",
       "        [0.63427091, 0.62714236, 0.58368708, 0.62940952, 0.01703709,\n",
       "         0.46487166],\n",
       "        [0.65973353, 0.60405736, 0.59085052, 0.5       , 0.        ,\n",
       "         0.45706947]],\n",
       "\n",
       "       [[0.62057735, 0.59706191, 0.62738781, 0.75      , 0.0669873 ,\n",
       "         0.45063804],\n",
       "        [0.63427091, 0.62714236, 0.58368708, 0.62940952, 0.01703709,\n",
       "         0.46487166],\n",
       "        [0.65973353, 0.60405736, 0.59085052, 0.5       , 0.        ,\n",
       "         0.45706947],\n",
       "        [0.65751295, 0.61874781, 0.60851273, 0.37059048, 0.01703709,\n",
       "         0.45408418]],\n",
       "\n",
       "       [[0.63427091, 0.62714236, 0.58368708, 0.62940952, 0.01703709,\n",
       "         0.46487166],\n",
       "        [0.65973353, 0.60405736, 0.59085052, 0.5       , 0.        ,\n",
       "         0.45706947],\n",
       "        [0.65751295, 0.61874781, 0.60851273, 0.37059048, 0.01703709,\n",
       "         0.45408418],\n",
       "        [0.67483346, 0.62609304, 0.65782292, 0.25      , 0.0669873 ,\n",
       "         0.44936402]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15408738],\n",
       "       [0.15618463],\n",
       "       [0.16649471],\n",
       "       ...,\n",
       "       [0.45408418],\n",
       "       [0.44936402],\n",
       "       [0.4394038 ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Define the model configuration\n",
    "model_config = {\n",
    "  'cnn_layers': [\n",
    "        {'filters': 32, 'kernel_size': 3, 'activation': 'relu', 'pool_size': 2, 'dropout': 0.3}, #utput shape: (None, 3, 64)\n",
    "    ],\n",
    "    'epochs': 60,\n",
    "    'batch_size': 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Samples: 28716\n",
      "Num Time Steps: 4\n",
      "Num Features: 6\n"
     ]
    }
   ],
   "source": [
    "xs_train_scaled.shape\n",
    "print('Num Samples:', xs_train_scaled.shape[0])\n",
    "print('Num Time Steps:', xs_train_scaled.shape[1])\n",
    "print('Num Features:', xs_train_scaled.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 4, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m172/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293us/step - loss: 0.1578  \n",
      "Epoch 1: loss improved from inf to 0.04215, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step - loss: 0.1031 - val_loss: 0.0039\n",
      "Epoch 2/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0128\n",
      "Epoch 2: loss improved from 0.04215 to 0.01138, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333us/step - loss: 0.0124 - val_loss: 0.0028\n",
      "Epoch 3/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246us/step - loss: 0.0092\n",
      "Epoch 3: loss improved from 0.01138 to 0.00828, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334us/step - loss: 0.0089 - val_loss: 0.0020\n",
      "Epoch 4/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step - loss: 0.0073\n",
      "Epoch 4: loss improved from 0.00828 to 0.00649, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342us/step - loss: 0.0071 - val_loss: 0.0016\n",
      "Epoch 5/100\n",
      "\u001b[1m340/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447us/step - loss: 0.0055\n",
      "Epoch 5: loss improved from 0.00649 to 0.00526, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - loss: 0.0055 - val_loss: 0.0018\n",
      "Epoch 6/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244us/step - loss: 0.0045\n",
      "Epoch 6: loss improved from 0.00526 to 0.00441, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333us/step - loss: 0.0045 - val_loss: 0.0013\n",
      "Epoch 7/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246us/step - loss: 0.0042\n",
      "Epoch 7: loss improved from 0.00441 to 0.00400, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332us/step - loss: 0.0041 - val_loss: 0.0014\n",
      "Epoch 8/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0037\n",
      "Epoch 8: loss improved from 0.00400 to 0.00368, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323us/step - loss: 0.0037 - val_loss: 0.0014\n",
      "Epoch 9/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0037\n",
      "Epoch 9: loss improved from 0.00368 to 0.00361, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344us/step - loss: 0.0037 - val_loss: 0.0013\n",
      "Epoch 10/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0034\n",
      "Epoch 10: loss improved from 0.00361 to 0.00331, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324us/step - loss: 0.0034 - val_loss: 0.0016\n",
      "Epoch 11/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0030\n",
      "Epoch 11: loss improved from 0.00331 to 0.00308, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0030 - val_loss: 0.0015\n",
      "Epoch 12/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0031\n",
      "Epoch 12: loss improved from 0.00308 to 0.00302, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325us/step - loss: 0.0031 - val_loss: 0.0014\n",
      "Epoch 13/100\n",
      "\u001b[1m199/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254us/step - loss: 0.0029\n",
      "Epoch 13: loss improved from 0.00302 to 0.00290, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step - loss: 0.0029 - val_loss: 0.0013\n",
      "Epoch 14/100\n",
      "\u001b[1m197/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255us/step - loss: 0.0029\n",
      "Epoch 14: loss improved from 0.00290 to 0.00285, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - loss: 0.0029 - val_loss: 0.0012\n",
      "Epoch 15/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0028\n",
      "Epoch 15: loss improved from 0.00285 to 0.00280, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342us/step - loss: 0.0028 - val_loss: 0.0012\n",
      "Epoch 16/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0029\n",
      "Epoch 16: loss improved from 0.00280 to 0.00277, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0028 - val_loss: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m202/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249us/step - loss: 0.0027\n",
      "Epoch 17: loss improved from 0.00277 to 0.00272, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 0.0027 - val_loss: 0.0012\n",
      "Epoch 18/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244us/step - loss: 0.0027\n",
      "Epoch 18: loss improved from 0.00272 to 0.00271, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338us/step - loss: 0.0027 - val_loss: 0.0014\n",
      "Epoch 19/100\n",
      "\u001b[1m325/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 0.0027\n",
      "Epoch 19: loss improved from 0.00271 to 0.00265, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.0027 - val_loss: 9.3581e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0027\n",
      "Epoch 20: loss improved from 0.00265 to 0.00264, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 0.0027 - val_loss: 0.0012\n",
      "Epoch 21/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0025\n",
      "Epoch 21: loss improved from 0.00264 to 0.00251, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326us/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 22/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0025\n",
      "Epoch 22: loss improved from 0.00251 to 0.00246, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327us/step - loss: 0.0025 - val_loss: 9.5222e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - loss: 0.0025\n",
      "Epoch 23: loss did not improve from 0.00246\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0025 - val_loss: 0.0015\n",
      "Epoch 24/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0025\n",
      "Epoch 24: loss improved from 0.00246 to 0.00244, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0025 - val_loss: 7.3563e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - loss: 0.0024\n",
      "Epoch 25: loss did not improve from 0.00244\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303us/step - loss: 0.0024 - val_loss: 6.6154e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0025\n",
      "Epoch 26: loss did not improve from 0.00244\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299us/step - loss: 0.0025 - val_loss: 8.4280e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0024\n",
      "Epoch 27: loss improved from 0.00244 to 0.00241, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.0024 - val_loss: 9.3323e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m193/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261us/step - loss: 0.0025\n",
      "Epoch 28: loss improved from 0.00241 to 0.00240, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - loss: 0.0025 - val_loss: 7.6279e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0024\n",
      "Epoch 29: loss improved from 0.00240 to 0.00238, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322us/step - loss: 0.0024 - val_loss: 7.3352e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0024\n",
      "Epoch 30: loss improved from 0.00238 to 0.00235, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325us/step - loss: 0.0024 - val_loss: 7.4054e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m190/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265us/step - loss: 0.0024\n",
      "Epoch 31: loss did not improve from 0.00235\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - loss: 0.0024 - val_loss: 5.6540e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0024\n",
      "Epoch 32: loss did not improve from 0.00235\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298us/step - loss: 0.0024 - val_loss: 8.3329e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m217/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233us/step - loss: 0.0024\n",
      "Epoch 33: loss improved from 0.00235 to 0.00234, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318us/step - loss: 0.0024 - val_loss: 8.3074e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - loss: 0.0023\n",
      "Epoch 34: loss improved from 0.00234 to 0.00232, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 35/100\n",
      "\u001b[1m217/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232us/step - loss: 0.0023\n",
      "Epoch 35: loss did not improve from 0.00232\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294us/step - loss: 0.0023 - val_loss: 7.4201e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step - loss: 0.0023\n",
      "Epoch 36: loss improved from 0.00232 to 0.00230, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338us/step - loss: 0.0023 - val_loss: 8.4018e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m216/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233us/step - loss: 0.0023\n",
      "Epoch 37: loss did not improve from 0.00230\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294us/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 38/100\n",
      "\u001b[1m218/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231us/step - loss: 0.0023\n",
      "Epoch 38: loss improved from 0.00230 to 0.00226, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334us/step - loss: 0.0023 - val_loss: 7.3930e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m218/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231us/step - loss: 0.0022\n",
      "Epoch 39: loss improved from 0.00226 to 0.00225, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333us/step - loss: 0.0022 - val_loss: 6.8480e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m216/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233us/step - loss: 0.0022\n",
      "Epoch 40: loss improved from 0.00225 to 0.00220, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320us/step - loss: 0.0022 - val_loss: 8.9543e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m221/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228us/step - loss: 0.0022\n",
      "Epoch 41: loss did not improve from 0.00220\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359us/step - loss: 0.0022 - val_loss: 6.3723e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m160/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step - loss: 0.0022\n",
      "Epoch 42: loss did not improve from 0.00220\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step - loss: 0.0022 - val_loss: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - loss: 0.0023\n",
      "Epoch 43: loss did not improve from 0.00220\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299us/step - loss: 0.0023 - val_loss: 7.3337e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0023\n",
      "Epoch 44: loss did not improve from 0.00220\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297us/step - loss: 0.0023 - val_loss: 7.3501e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m216/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233us/step - loss: 0.0022\n",
      "Epoch 45: loss did not improve from 0.00220\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295us/step - loss: 0.0022 - val_loss: 6.6414e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m216/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233us/step - loss: 0.0023\n",
      "Epoch 46: loss did not improve from 0.00220\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293us/step - loss: 0.0022 - val_loss: 8.0552e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m219/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230us/step - loss: 0.0022\n",
      "Epoch 47: loss did not improve from 0.00220\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290us/step - loss: 0.0022 - val_loss: 9.2727e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m216/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233us/step - loss: 0.0022\n",
      "Epoch 48: loss improved from 0.00220 to 0.00218, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335us/step - loss: 0.0022 - val_loss: 6.4958e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m218/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231us/step - loss: 0.0023\n",
      "Epoch 49: loss did not improve from 0.00218\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292us/step - loss: 0.0023 - val_loss: 7.7823e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m217/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232us/step - loss: 0.0021\n",
      "Epoch 50: loss improved from 0.00218 to 0.00214, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334us/step - loss: 0.0021 - val_loss: 6.8693e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488us/step - loss: 0.0022\n",
      "Epoch 51: loss did not improve from 0.00214\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480us/step - loss: 0.0022 - val_loss: 5.3694e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0022\n",
      "Epoch 52: loss did not improve from 0.00214\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296us/step - loss: 0.0022 - val_loss: 5.2869e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m217/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232us/step - loss: 0.0022\n",
      "Epoch 53: loss did not improve from 0.00214\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293us/step - loss: 0.0022 - val_loss: 5.4582e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m177/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284us/step - loss: 0.0023\n",
      "Epoch 54: loss did not improve from 0.00214\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329us/step - loss: 0.0023 - val_loss: 8.3746e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m214/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235us/step - loss: 0.0021\n",
      "Epoch 55: loss did not improve from 0.00214\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296us/step - loss: 0.0021 - val_loss: 7.0330e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - loss: 0.0022\n",
      "Epoch 56: loss did not improve from 0.00214\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 0.0021 - val_loss: 6.8838e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - loss: 0.0021\n",
      "Epoch 57: loss did not improve from 0.00214\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0021 - val_loss: 5.1274e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m202/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249us/step - loss: 0.0021\n",
      "Epoch 58: loss did not improve from 0.00214\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step - loss: 0.0021 - val_loss: 7.3713e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0023\n",
      "Epoch 59: loss did not improve from 0.00214\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - loss: 0.0022 - val_loss: 5.8195e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m197/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255us/step - loss: 0.0021\n",
      "Epoch 60: loss improved from 0.00214 to 0.00212, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step - loss: 0.0021 - val_loss: 7.5501e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m215/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234us/step - loss: 0.0023\n",
      "Epoch 61: loss did not improve from 0.00212\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294us/step - loss: 0.0022 - val_loss: 7.6743e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m217/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233us/step - loss: 0.0021\n",
      "Epoch 62: loss did not improve from 0.00212\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step - loss: 0.0022 - val_loss: 9.5757e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m215/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234us/step - loss: 0.0021\n",
      "Epoch 63: loss improved from 0.00212 to 0.00212, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342us/step - loss: 0.0021 - val_loss: 7.4131e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0021\n",
      "Epoch 64: loss did not improve from 0.00212\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323us/step - loss: 0.0021 - val_loss: 9.8971e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m342/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294us/step - loss: 0.0022\n",
      "Epoch 65: loss did not improve from 0.00212\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step - loss: 0.0022 - val_loss: 7.0688e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m325/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309us/step - loss: 0.0022\n",
      "Epoch 66: loss did not improve from 0.00212\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370us/step - loss: 0.0022 - val_loss: 5.6571e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0021\n",
      "Epoch 67: loss did not improve from 0.00212\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - loss: 0.0021 - val_loss: 6.5066e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m214/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235us/step - loss: 0.0022\n",
      "Epoch 68: loss did not improve from 0.00212\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298us/step - loss: 0.0022 - val_loss: 6.5899e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0021\n",
      "Epoch 69: loss did not improve from 0.00212\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320us/step - loss: 0.0021 - val_loss: 0.0010\n",
      "Epoch 70/100\n",
      "\u001b[1m216/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233us/step - loss: 0.0021\n",
      "Epoch 70: loss did not improve from 0.00212\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294us/step - loss: 0.0021 - val_loss: 9.3393e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m218/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231us/step - loss: 0.0022\n",
      "Epoch 71: loss did not improve from 0.00212\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296us/step - loss: 0.0021 - val_loss: 6.2095e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m214/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235us/step - loss: 0.0022\n",
      "Epoch 72: loss did not improve from 0.00212\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298us/step - loss: 0.0022 - val_loss: 4.8103e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m217/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232us/step - loss: 0.0021\n",
      "Epoch 73: loss did not improve from 0.00212\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295us/step - loss: 0.0021 - val_loss: 6.5964e-04\n",
      "Epoch 73: early stopping\n",
      "Restoring model weights from the end of the best epoch: 63.\n",
      "Epoch 1/100\n",
      "\u001b[1m321/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - loss: 0.0438 \n",
      "Epoch 1: loss improved from inf to 0.02384, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.0417 - val_loss: 0.0048\n",
      "Epoch 2/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244us/step - loss: 0.0104\n",
      "Epoch 2: loss improved from 0.02384 to 0.00850, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - loss: 0.0098 - val_loss: 0.0028\n",
      "Epoch 3/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0060\n",
      "Epoch 3: loss improved from 0.00850 to 0.00539, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423us/step - loss: 0.0058 - val_loss: 0.0019\n",
      "Epoch 4/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0047\n",
      "Epoch 4: loss improved from 0.00539 to 0.00432, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.0046 - val_loss: 0.0020\n",
      "Epoch 5/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0038\n",
      "Epoch 5: loss improved from 0.00432 to 0.00376, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 0.0038 - val_loss: 0.0018\n",
      "Epoch 6/100\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280us/step - loss: 0.0034\n",
      "Epoch 6: loss improved from 0.00376 to 0.00346, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394us/step - loss: 0.0034 - val_loss: 0.0019\n",
      "Epoch 7/100\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280us/step - loss: 0.0032\n",
      "Epoch 7: loss improved from 0.00346 to 0.00324, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391us/step - loss: 0.0032 - val_loss: 0.0019\n",
      "Epoch 8/100\n",
      "\u001b[1m179/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281us/step - loss: 0.0031\n",
      "Epoch 8: loss improved from 0.00324 to 0.00306, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 9/100\n",
      "\u001b[1m333/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0030\n",
      "Epoch 9: loss improved from 0.00306 to 0.00295, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - loss: 0.0030 - val_loss: 0.0016\n",
      "Epoch 10/100\n",
      "\u001b[1m199/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254us/step - loss: 0.0029\n",
      "Epoch 10: loss improved from 0.00295 to 0.00285, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349us/step - loss: 0.0029 - val_loss: 0.0013\n",
      "Epoch 11/100\n",
      "\u001b[1m330/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459us/step - loss: 0.0028\n",
      "Epoch 11: loss improved from 0.00285 to 0.00283, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - loss: 0.0028 - val_loss: 0.0013\n",
      "Epoch 12/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0028\n",
      "Epoch 12: loss improved from 0.00283 to 0.00274, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328us/step - loss: 0.0028 - val_loss: 0.0014\n",
      "Epoch 13/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0027\n",
      "Epoch 13: loss improved from 0.00274 to 0.00269, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step - loss: 0.0027 - val_loss: 0.0012\n",
      "Epoch 14/100\n",
      "\u001b[1m188/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268us/step - loss: 0.0025\n",
      "Epoch 14: loss improved from 0.00269 to 0.00263, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421us/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 15/100\n",
      "\u001b[1m182/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277us/step - loss: 0.0027\n",
      "Epoch 15: loss improved from 0.00263 to 0.00254, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383us/step - loss: 0.0026 - val_loss: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m181/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279us/step - loss: 0.0026\n",
      "Epoch 16: loss did not improve from 0.00254\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360us/step - loss: 0.0026 - val_loss: 9.6312e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m344/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293us/step - loss: 0.0026\n",
      "Epoch 17: loss did not improve from 0.00254\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - loss: 0.0026 - val_loss: 0.0011\n",
      "Epoch 18/100\n",
      "\u001b[1m339/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 0.0026\n",
      "Epoch 18: loss did not improve from 0.00254\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 0.0026 - val_loss: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m358/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281us/step - loss: 0.0024\n",
      "Epoch 19: loss improved from 0.00254 to 0.00248, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472us/step - loss: 0.0024 - val_loss: 0.0012\n",
      "Epoch 20/100\n",
      "\u001b[1m315/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319us/step - loss: 0.0025\n",
      "Epoch 20: loss improved from 0.00248 to 0.00246, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480us/step - loss: 0.0025 - val_loss: 0.0013\n",
      "Epoch 21/100\n",
      "\u001b[1m200/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252us/step - loss: 0.0025\n",
      "Epoch 21: loss did not improve from 0.00246\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - loss: 0.0025 - val_loss: 9.6352e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m353/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285us/step - loss: 0.0025\n",
      "Epoch 22: loss did not improve from 0.00246\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442us/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 23/100\n",
      "\u001b[1m182/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277us/step - loss: 0.0025\n",
      "Epoch 23: loss improved from 0.00246 to 0.00242, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404us/step - loss: 0.0025 - val_loss: 8.0433e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m182/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276us/step - loss: 0.0024\n",
      "Epoch 24: loss improved from 0.00242 to 0.00242, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - loss: 0.0024 - val_loss: 7.5428e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m201/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251us/step - loss: 0.0024\n",
      "Epoch 25: loss improved from 0.00242 to 0.00234, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338us/step - loss: 0.0024 - val_loss: 8.5318e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step - loss: 0.0024\n",
      "Epoch 26: loss did not improve from 0.00234\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - loss: 0.0024 - val_loss: 8.2373e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0023\n",
      "Epoch 27: loss did not improve from 0.00234\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - loss: 0.0023 - val_loss: 9.9660e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m217/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232us/step - loss: 0.0023\n",
      "Epoch 28: loss improved from 0.00234 to 0.00232, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.0023 - val_loss: 9.3675e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m185/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273us/step - loss: 0.0023\n",
      "Epoch 29: loss did not improve from 0.00232\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350us/step - loss: 0.0023 - val_loss: 7.9191e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m331/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - loss: 0.0023\n",
      "Epoch 30: loss did not improve from 0.00232\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.0023 - val_loss: 7.0178e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step - loss: 0.0024\n",
      "Epoch 31: loss did not improve from 0.00232\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322us/step - loss: 0.0024 - val_loss: 9.6227e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0023\n",
      "Epoch 32: loss improved from 0.00232 to 0.00231, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - loss: 0.0023 - val_loss: 7.7606e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m215/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234us/step - loss: 0.0024\n",
      "Epoch 33: loss did not improve from 0.00231\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299us/step - loss: 0.0024 - val_loss: 8.3088e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0023\n",
      "Epoch 34: loss did not improve from 0.00231\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301us/step - loss: 0.0023 - val_loss: 7.1569e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m218/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231us/step - loss: 0.0022\n",
      "Epoch 35: loss improved from 0.00231 to 0.00224, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329us/step - loss: 0.0022 - val_loss: 7.0204e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0023\n",
      "Epoch 36: loss did not improve from 0.00224\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297us/step - loss: 0.0023 - val_loss: 7.0907e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m215/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234us/step - loss: 0.0023\n",
      "Epoch 37: loss did not improve from 0.00224\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298us/step - loss: 0.0023 - val_loss: 6.6068e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step - loss: 0.0022\n",
      "Epoch 38: loss did not improve from 0.00224\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353us/step - loss: 0.0022 - val_loss: 6.6025e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m183/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276us/step - loss: 0.0023\n",
      "Epoch 39: loss did not improve from 0.00224\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353us/step - loss: 0.0023 - val_loss: 5.4629e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m193/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261us/step - loss: 0.0023\n",
      "Epoch 40: loss did not improve from 0.00224\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 0.0023 - val_loss: 6.0727e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m333/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0022\n",
      "Epoch 41: loss did not improve from 0.00224\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 429us/step - loss: 0.0022 - val_loss: 6.8647e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m186/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270us/step - loss: 0.0023\n",
      "Epoch 42: loss improved from 0.00224 to 0.00223, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406us/step - loss: 0.0022 - val_loss: 6.4976e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m189/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266us/step - loss: 0.0023\n",
      "Epoch 43: loss did not improve from 0.00223\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332us/step - loss: 0.0023 - val_loss: 5.6445e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m197/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256us/step - loss: 0.0023\n",
      "Epoch 44: loss did not improve from 0.00223\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324us/step - loss: 0.0023 - val_loss: 5.9150e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0022\n",
      "Epoch 45: loss did not improve from 0.00223\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323us/step - loss: 0.0022 - val_loss: 7.6021e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m187/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270us/step - loss: 0.0022\n",
      "Epoch 46: loss did not improve from 0.00223\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358us/step - loss: 0.0023 - val_loss: 5.7850e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m316/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319us/step - loss: 0.0022\n",
      "Epoch 47: loss did not improve from 0.00223\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418us/step - loss: 0.0022 - val_loss: 7.2603e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m246/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409us/step - loss: 0.0023\n",
      "Epoch 48: loss did not improve from 0.00223\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420us/step - loss: 0.0023 - val_loss: 5.9850e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m202/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249us/step - loss: 0.0023\n",
      "Epoch 49: loss did not improve from 0.00223\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332us/step - loss: 0.0023 - val_loss: 5.7720e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m358/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281us/step - loss: 0.0023\n",
      "Epoch 50: loss did not improve from 0.00223\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389us/step - loss: 0.0023 - val_loss: 6.2932e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m351/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 0.0022\n",
      "Epoch 51: loss improved from 0.00223 to 0.00218, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 0.0022 - val_loss: 7.0647e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m192/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263us/step - loss: 0.0022\n",
      "Epoch 52: loss did not improve from 0.00218\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 53/100\n",
      "\u001b[1m185/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273us/step - loss: 0.0023\n",
      "Epoch 53: loss did not improve from 0.00218\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 0.0022 - val_loss: 6.9609e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m186/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270us/step - loss: 0.0023\n",
      "Epoch 54: loss did not improve from 0.00218\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.0023 - val_loss: 7.5888e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m237/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424us/step - loss: 0.0021\n",
      "Epoch 55: loss did not improve from 0.00218\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 445us/step - loss: 0.0022 - val_loss: 5.8312e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m341/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295us/step - loss: 0.0023\n",
      "Epoch 56: loss did not improve from 0.00218\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364us/step - loss: 0.0023 - val_loss: 6.0764e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m193/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261us/step - loss: 0.0022\n",
      "Epoch 57: loss did not improve from 0.00218\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331us/step - loss: 0.0022 - val_loss: 8.3252e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m249/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - loss: 0.0022\n",
      "Epoch 58: loss did not improve from 0.00218\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582us/step - loss: 0.0022 - val_loss: 9.9302e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m186/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271us/step - loss: 0.0022\n",
      "Epoch 59: loss did not improve from 0.00218\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344us/step - loss: 0.0022 - val_loss: 9.0163e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m193/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261us/step - loss: 0.0022\n",
      "Epoch 60: loss did not improve from 0.00218\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331us/step - loss: 0.0022 - val_loss: 9.1331e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m197/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256us/step - loss: 0.0022\n",
      "Epoch 61: loss did not improve from 0.00218\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322us/step - loss: 0.0022 - val_loss: 8.2427e-04\n",
      "Epoch 61: early stopping\n",
      "Restoring model weights from the end of the best epoch: 51.\n",
      "Epoch 1/100\n",
      "\u001b[1m344/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293us/step - loss: 0.0254 \n",
      "Epoch 1: loss improved from inf to 0.01482, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547us/step - loss: 0.0249 - val_loss: 0.0029\n",
      "Epoch 2/100\n",
      "\u001b[1m192/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262us/step - loss: 0.0077\n",
      "Epoch 2: loss improved from 0.01482 to 0.00662, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405us/step - loss: 0.0073 - val_loss: 0.0021\n",
      "Epoch 3/100\n",
      "\u001b[1m292/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 0.0053\n",
      "Epoch 3: loss improved from 0.00662 to 0.00486, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442us/step - loss: 0.0052 - val_loss: 0.0025\n",
      "Epoch 4/100\n",
      "\u001b[1m354/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284us/step - loss: 0.0042\n",
      "Epoch 4: loss improved from 0.00486 to 0.00398, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384us/step - loss: 0.0042 - val_loss: 0.0024\n",
      "Epoch 5/100\n",
      "\u001b[1m191/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264us/step - loss: 0.0036\n",
      "Epoch 5: loss improved from 0.00398 to 0.00344, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350us/step - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 6/100\n",
      "\u001b[1m182/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276us/step - loss: 0.0031\n",
      "Epoch 6: loss improved from 0.00344 to 0.00311, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 7/100\n",
      "\u001b[1m194/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - loss: 0.0029\n",
      "Epoch 7: loss improved from 0.00311 to 0.00288, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - loss: 0.0029 - val_loss: 0.0012\n",
      "Epoch 8/100\n",
      "\u001b[1m196/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257us/step - loss: 0.0027\n",
      "Epoch 8: loss improved from 0.00288 to 0.00274, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 0.0027 - val_loss: 0.0013\n",
      "Epoch 9/100\n",
      "\u001b[1m196/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257us/step - loss: 0.0027\n",
      "Epoch 9: loss improved from 0.00274 to 0.00265, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400us/step - loss: 0.0027 - val_loss: 9.2867e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m158/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319us/step - loss: 0.0026\n",
      "Epoch 10: loss improved from 0.00265 to 0.00253, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399us/step - loss: 0.0026 - val_loss: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m200/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252us/step - loss: 0.0026\n",
      "Epoch 11: loss did not improve from 0.00253\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step - loss: 0.0026 - val_loss: 8.9052e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - loss: 0.0024\n",
      "Epoch 12: loss improved from 0.00253 to 0.00247, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358us/step - loss: 0.0025 - val_loss: 9.1165e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m201/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251us/step - loss: 0.0024\n",
      "Epoch 13: loss improved from 0.00247 to 0.00240, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step - loss: 0.0024 - val_loss: 0.0011\n",
      "Epoch 14/100\n",
      "\u001b[1m197/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255us/step - loss: 0.0025\n",
      "Epoch 14: loss improved from 0.00240 to 0.00240, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469us/step - loss: 0.0024 - val_loss: 9.2703e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m196/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257us/step - loss: 0.0024\n",
      "Epoch 15: loss improved from 0.00240 to 0.00231, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 0.0023 - val_loss: 8.3447e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m304/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331us/step - loss: 0.0023\n",
      "Epoch 16: loss improved from 0.00231 to 0.00230, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414us/step - loss: 0.0023 - val_loss: 8.6250e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m193/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260us/step - loss: 0.0023\n",
      "Epoch 17: loss improved from 0.00230 to 0.00225, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397us/step - loss: 0.0023 - val_loss: 8.4528e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0023\n",
      "Epoch 18: loss did not improve from 0.00225\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310us/step - loss: 0.0023 - val_loss: 8.8647e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246us/step - loss: 0.0022\n",
      "Epoch 19: loss improved from 0.00225 to 0.00223, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334us/step - loss: 0.0022 - val_loss: 6.9478e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m164/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - loss: 0.0023\n",
      "Epoch 20: loss improved from 0.00223 to 0.00221, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370us/step - loss: 0.0022 - val_loss: 8.3407e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m200/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251us/step - loss: 0.0021\n",
      "Epoch 21: loss improved from 0.00221 to 0.00217, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0021 - val_loss: 7.4905e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m293/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step - loss: 0.0022\n",
      "Epoch 22: loss improved from 0.00217 to 0.00214, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451us/step - loss: 0.0022 - val_loss: 8.4158e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m203/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step - loss: 0.0023\n",
      "Epoch 23: loss did not improve from 0.00214\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - loss: 0.0022 - val_loss: 5.7289e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0022\n",
      "Epoch 24: loss improved from 0.00214 to 0.00214, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398us/step - loss: 0.0022 - val_loss: 8.8593e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0022\n",
      "Epoch 25: loss did not improve from 0.00214\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - loss: 0.0022 - val_loss: 6.9965e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0021\n",
      "Epoch 26: loss improved from 0.00214 to 0.00212, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step - loss: 0.0021 - val_loss: 9.7179e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m192/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262us/step - loss: 0.0021\n",
      "Epoch 27: loss did not improve from 0.00212\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 481us/step - loss: 0.0022 - val_loss: 6.9158e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m192/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262us/step - loss: 0.0021\n",
      "Epoch 28: loss improved from 0.00212 to 0.00211, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342us/step - loss: 0.0021 - val_loss: 6.5721e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m203/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step - loss: 0.0020\n",
      "Epoch 29: loss improved from 0.00211 to 0.00209, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - loss: 0.0020 - val_loss: 6.8977e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m349/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288us/step - loss: 0.0021\n",
      "Epoch 30: loss did not improve from 0.00209\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - loss: 0.0021 - val_loss: 8.0184e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0022\n",
      "Epoch 31: loss did not improve from 0.00209\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0022 - val_loss: 6.4523e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - loss: 0.0021\n",
      "Epoch 32: loss did not improve from 0.00209\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303us/step - loss: 0.0021 - val_loss: 5.9146e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m177/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285us/step - loss: 0.0022\n",
      "Epoch 33: loss did not improve from 0.00209\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324us/step - loss: 0.0022 - val_loss: 6.7917e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - loss: 0.0021\n",
      "Epoch 34: loss did not improve from 0.00209\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - loss: 0.0021 - val_loss: 8.7693e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m214/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235us/step - loss: 0.0022\n",
      "Epoch 35: loss improved from 0.00209 to 0.00208, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359us/step - loss: 0.0022 - val_loss: 6.3302e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m352/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432us/step - loss: 0.0021\n",
      "Epoch 36: loss did not improve from 0.00208\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495us/step - loss: 0.0021 - val_loss: 5.3383e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246us/step - loss: 0.0020\n",
      "Epoch 37: loss improved from 0.00208 to 0.00207, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step - loss: 0.0020 - val_loss: 5.6736e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246us/step - loss: 0.0021\n",
      "Epoch 38: loss did not improve from 0.00207\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - loss: 0.0021 - val_loss: 7.7975e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m318/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316us/step - loss: 0.0020\n",
      "Epoch 39: loss improved from 0.00207 to 0.00206, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 417us/step - loss: 0.0020 - val_loss: 8.0057e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m201/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251us/step - loss: 0.0021\n",
      "Epoch 40: loss did not improve from 0.00206\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311us/step - loss: 0.0021 - val_loss: 5.6638e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m181/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279us/step - loss: 0.0020\n",
      "Epoch 41: loss did not improve from 0.00206\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319us/step - loss: 0.0020 - val_loss: 7.1328e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - loss: 0.0021\n",
      "Epoch 42: loss did not improve from 0.00206\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301us/step - loss: 0.0021 - val_loss: 4.8904e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m214/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step - loss: 0.0020\n",
      "Epoch 43: loss did not improve from 0.00206\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299us/step - loss: 0.0020 - val_loss: 5.4995e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0020\n",
      "Epoch 44: loss improved from 0.00206 to 0.00204, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360us/step - loss: 0.0020 - val_loss: 7.2355e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0020\n",
      "Epoch 45: loss did not improve from 0.00204\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - loss: 0.0021 - val_loss: 7.3573e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step - loss: 0.0019\n",
      "Epoch 46: loss improved from 0.00204 to 0.00203, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364us/step - loss: 0.0020 - val_loss: 7.7540e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0020\n",
      "Epoch 47: loss did not improve from 0.00203\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - loss: 0.0020 - val_loss: 5.5889e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m323/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311us/step - loss: 0.0020\n",
      "Epoch 48: loss did not improve from 0.00203\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step - loss: 0.0020 - val_loss: 8.5049e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0020\n",
      "Epoch 49: loss did not improve from 0.00203\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - loss: 0.0020 - val_loss: 6.0852e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m214/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235us/step - loss: 0.0020\n",
      "Epoch 50: loss did not improve from 0.00203\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329us/step - loss: 0.0020 - val_loss: 7.2693e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m228/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448us/step - loss: 0.0022\n",
      "Epoch 51: loss did not improve from 0.00203\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479us/step - loss: 0.0022 - val_loss: 9.2651e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - loss: 0.0021\n",
      "Epoch 52: loss did not improve from 0.00203\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0021 - val_loss: 7.7297e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0021\n",
      "Epoch 53: loss did not improve from 0.00203\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - loss: 0.0021 - val_loss: 5.2583e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step - loss: 0.0021\n",
      "Epoch 54: loss improved from 0.00203 to 0.00201, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step - loss: 0.0021 - val_loss: 5.8399e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0020\n",
      "Epoch 55: loss improved from 0.00201 to 0.00199, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step - loss: 0.0020 - val_loss: 7.4832e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0020\n",
      "Epoch 56: loss did not improve from 0.00199\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - loss: 0.0020 - val_loss: 7.9798e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m328/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - loss: 0.0020\n",
      "Epoch 57: loss did not improve from 0.00199\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367us/step - loss: 0.0020 - val_loss: 7.0674e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m347/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438us/step - loss: 0.0021\n",
      "Epoch 58: loss did not improve from 0.00199\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step - loss: 0.0021 - val_loss: 5.4889e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - loss: 0.0020\n",
      "Epoch 59: loss improved from 0.00199 to 0.00199, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332us/step - loss: 0.0020 - val_loss: 5.8693e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m200/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251us/step - loss: 0.0021\n",
      "Epoch 60: loss did not improve from 0.00199\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309us/step - loss: 0.0020 - val_loss: 5.5638e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0020\n",
      "Epoch 61: loss did not improve from 0.00199\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301us/step - loss: 0.0020 - val_loss: 6.1362e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step - loss: 0.0020\n",
      "Epoch 62: loss did not improve from 0.00199\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0020 - val_loss: 5.0822e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m214/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step - loss: 0.0021\n",
      "Epoch 63: loss did not improve from 0.00199\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - loss: 0.0021 - val_loss: 7.1182e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m214/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235us/step - loss: 0.0021\n",
      "Epoch 64: loss did not improve from 0.00199\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299us/step - loss: 0.0021 - val_loss: 5.7411e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m215/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235us/step - loss: 0.0020\n",
      "Epoch 65: loss did not improve from 0.00199\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298us/step - loss: 0.0020 - val_loss: 5.8132e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m214/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step - loss: 0.0021\n",
      "Epoch 66: loss did not improve from 0.00199\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298us/step - loss: 0.0021 - val_loss: 6.7203e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m331/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - loss: 0.0022\n",
      "Epoch 67: loss did not improve from 0.00199\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364us/step - loss: 0.0022 - val_loss: 5.6180e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0021\n",
      "Epoch 68: loss improved from 0.00199 to 0.00199, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389us/step - loss: 0.0020 - val_loss: 7.0194e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m178/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284us/step - loss: 0.0020\n",
      "Epoch 69: loss did not improve from 0.00199\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327us/step - loss: 0.0020 - val_loss: 6.0126e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0022\n",
      "Epoch 70: loss did not improve from 0.00199\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - loss: 0.0022 - val_loss: 6.0191e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0020\n",
      "Epoch 71: loss did not improve from 0.00199\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - loss: 0.0020 - val_loss: 5.1304e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m214/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235us/step - loss: 0.0020\n",
      "Epoch 72: loss did not improve from 0.00199\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - loss: 0.0020 - val_loss: 5.2838e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m169/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - loss: 0.0020\n",
      "Epoch 73: loss did not improve from 0.00199\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step - loss: 0.0020 - val_loss: 5.0690e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m320/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315us/step - loss: 0.0020\n",
      "Epoch 74: loss did not improve from 0.00199\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372us/step - loss: 0.0020 - val_loss: 7.8965e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m171/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294us/step - loss: 0.0020\n",
      "Epoch 75: loss improved from 0.00199 to 0.00197, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.0020 - val_loss: 5.0493e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246us/step - loss: 0.0021\n",
      "Epoch 76: loss did not improve from 0.00197\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - loss: 0.0020 - val_loss: 6.3866e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0021\n",
      "Epoch 77: loss did not improve from 0.00197\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - loss: 0.0020 - val_loss: 5.1759e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0019\n",
      "Epoch 78: loss did not improve from 0.00197\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316us/step - loss: 0.0020 - val_loss: 6.6727e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m186/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271us/step - loss: 0.0020\n",
      "Epoch 79: loss did not improve from 0.00197\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step - loss: 0.0020 - val_loss: 5.5929e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m187/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270us/step - loss: 0.0021\n",
      "Epoch 80: loss did not improve from 0.00197\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338us/step - loss: 0.0020 - val_loss: 4.9275e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m251/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 610us/step - loss: 0.0020\n",
      "Epoch 81: loss did not improve from 0.00197\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 0.0020 - val_loss: 4.7936e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m220/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461us/step - loss: 0.0020\n",
      "Epoch 82: loss did not improve from 0.00197\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 449us/step - loss: 0.0020 - val_loss: 4.5728e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244us/step - loss: 0.0020\n",
      "Epoch 83: loss did not improve from 0.00197\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316us/step - loss: 0.0020 - val_loss: 7.8445e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m184/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274us/step - loss: 0.0020\n",
      "Epoch 84: loss did not improve from 0.00197\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0021 - val_loss: 4.9950e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m193/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261us/step - loss: 0.0019\n",
      "Epoch 85: loss did not improve from 0.00197\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340us/step - loss: 0.0019 - val_loss: 6.2291e-04\n",
      "Epoch 85: early stopping\n",
      "Restoring model weights from the end of the best epoch: 75.\n",
      "Epoch 1/100\n",
      "\u001b[1m173/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292us/step - loss: 0.0596  \n",
      "Epoch 1: loss improved from inf to 0.02332, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step - loss: 0.0432 - val_loss: 0.0049\n",
      "Epoch 2/100\n",
      "\u001b[1m180/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280us/step - loss: 0.0096\n",
      "Epoch 2: loss improved from 0.02332 to 0.00798, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424us/step - loss: 0.0090 - val_loss: 0.0027\n",
      "Epoch 3/100\n",
      "\u001b[1m322/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step - loss: 0.0061\n",
      "Epoch 3: loss improved from 0.00798 to 0.00566, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436us/step - loss: 0.0061 - val_loss: 0.0021\n",
      "Epoch 4/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0046\n",
      "Epoch 4: loss improved from 0.00566 to 0.00434, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434us/step - loss: 0.0045 - val_loss: 0.0021\n",
      "Epoch 5/100\n",
      "\u001b[1m347/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290us/step - loss: 0.0039\n",
      "Epoch 5: loss improved from 0.00434 to 0.00381, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405us/step - loss: 0.0039 - val_loss: 0.0017\n",
      "Epoch 6/100\n",
      "\u001b[1m274/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 0.0035\n",
      "Epoch 6: loss improved from 0.00381 to 0.00343, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 7/100\n",
      "\u001b[1m187/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269us/step - loss: 0.0034\n",
      "Epoch 7: loss improved from 0.00343 to 0.00324, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 427us/step - loss: 0.0033 - val_loss: 0.0015\n",
      "Epoch 8/100\n",
      "\u001b[1m197/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255us/step - loss: 0.0030\n",
      "Epoch 8: loss improved from 0.00324 to 0.00309, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - loss: 0.0031 - val_loss: 0.0014\n",
      "Epoch 9/100\n",
      "\u001b[1m345/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291us/step - loss: 0.0030\n",
      "Epoch 9: loss improved from 0.00309 to 0.00296, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400us/step - loss: 0.0030 - val_loss: 0.0014\n",
      "Epoch 10/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0027\n",
      "Epoch 10: loss improved from 0.00296 to 0.00281, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step - loss: 0.0027 - val_loss: 0.0012\n",
      "Epoch 11/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244us/step - loss: 0.0027\n",
      "Epoch 11: loss improved from 0.00281 to 0.00267, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336us/step - loss: 0.0027 - val_loss: 0.0016\n",
      "Epoch 12/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0028\n",
      "Epoch 12: loss improved from 0.00267 to 0.00260, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - loss: 0.0027 - val_loss: 0.0012\n",
      "Epoch 13/100\n",
      "\u001b[1m162/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - loss: 0.0024\n",
      "Epoch 13: loss improved from 0.00260 to 0.00251, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 0.0024 - val_loss: 0.0011\n",
      "Epoch 14/100\n",
      "\u001b[1m202/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250us/step - loss: 0.0024\n",
      "Epoch 14: loss improved from 0.00251 to 0.00246, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0024 - val_loss: 0.0013\n",
      "Epoch 15/100\n",
      "\u001b[1m219/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229us/step - loss: 0.0024\n",
      "Epoch 15: loss improved from 0.00246 to 0.00243, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - loss: 0.0024 - val_loss: 0.0013\n",
      "Epoch 16/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0025\n",
      "Epoch 16: loss improved from 0.00243 to 0.00242, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384us/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 17/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0024\n",
      "Epoch 17: loss improved from 0.00242 to 0.00236, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - loss: 0.0024 - val_loss: 0.0012\n",
      "Epoch 18/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0025\n",
      "Epoch 18: loss did not improve from 0.00236\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - loss: 0.0024 - val_loss: 0.0011\n",
      "Epoch 19/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0023\n",
      "Epoch 19: loss did not improve from 0.00236\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - loss: 0.0024 - val_loss: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0023\n",
      "Epoch 20: loss improved from 0.00236 to 0.00231, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334us/step - loss: 0.0023 - val_loss: 0.0011\n",
      "Epoch 21/100\n",
      "\u001b[1m221/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228us/step - loss: 0.0023\n",
      "Epoch 21: loss did not improve from 0.00231\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296us/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 22/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0023\n",
      "Epoch 22: loss improved from 0.00231 to 0.00228, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step - loss: 0.0023 - val_loss: 8.4971e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0023\n",
      "Epoch 23: loss did not improve from 0.00228\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - loss: 0.0023 - val_loss: 8.8947e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0023\n",
      "Epoch 24: loss did not improve from 0.00228\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0023 - val_loss: 9.5868e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0023\n",
      "Epoch 25: loss improved from 0.00228 to 0.00224, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340us/step - loss: 0.0023 - val_loss: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m336/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 449us/step - loss: 0.0021\n",
      "Epoch 26: loss improved from 0.00224 to 0.00217, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 0.0021 - val_loss: 6.9606e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m182/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278us/step - loss: 0.0023\n",
      "Epoch 27: loss did not improve from 0.00217\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336us/step - loss: 0.0022 - val_loss: 7.3619e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m195/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258us/step - loss: 0.0021\n",
      "Epoch 28: loss did not improve from 0.00217\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313us/step - loss: 0.0022 - val_loss: 9.1308e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step - loss: 0.0022\n",
      "Epoch 29: loss did not improve from 0.00217\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298us/step - loss: 0.0022 - val_loss: 6.9086e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - loss: 0.0022\n",
      "Epoch 30: loss did not improve from 0.00217\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411us/step - loss: 0.0022 - val_loss: 7.0432e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m199/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253us/step - loss: 0.0021\n",
      "Epoch 31: loss improved from 0.00217 to 0.00215, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379us/step - loss: 0.0022 - val_loss: 9.3675e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m182/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277us/step - loss: 0.0023\n",
      "Epoch 32: loss did not improve from 0.00215\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383us/step - loss: 0.0022 - val_loss: 6.4339e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m342/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294us/step - loss: 0.0022\n",
      "Epoch 33: loss improved from 0.00215 to 0.00215, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420us/step - loss: 0.0022 - val_loss: 8.7543e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m340/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - loss: 0.0022\n",
      "Epoch 34: loss improved from 0.00215 to 0.00214, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 0.0022 - val_loss: 9.9741e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m181/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278us/step - loss: 0.0021\n",
      "Epoch 35: loss did not improve from 0.00214\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 0.0022 - val_loss: 6.2500e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m344/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292us/step - loss: 0.0020\n",
      "Epoch 36: loss improved from 0.00214 to 0.00207, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404us/step - loss: 0.0020 - val_loss: 6.7057e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m180/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279us/step - loss: 0.0023\n",
      "Epoch 37: loss did not improve from 0.00207\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 0.0022 - val_loss: 5.9673e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m355/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284us/step - loss: 0.0020\n",
      "Epoch 38: loss did not improve from 0.00207\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 443us/step - loss: 0.0020 - val_loss: 6.9755e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m292/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 0.0020\n",
      "Epoch 39: loss improved from 0.00207 to 0.00203, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step - loss: 0.0020 - val_loss: 5.9306e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280us/step - loss: 0.0021\n",
      "Epoch 40: loss did not improve from 0.00203\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396us/step - loss: 0.0021 - val_loss: 6.1939e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m334/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301us/step - loss: 0.0021\n",
      "Epoch 41: loss did not improve from 0.00203\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382us/step - loss: 0.0021 - val_loss: 6.0996e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m221/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455us/step - loss: 0.0021\n",
      "Epoch 42: loss did not improve from 0.00203\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step - loss: 0.0021 - val_loss: 7.4439e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m203/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step - loss: 0.0021\n",
      "Epoch 43: loss did not improve from 0.00203\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311us/step - loss: 0.0021 - val_loss: 5.3872e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244us/step - loss: 0.0022\n",
      "Epoch 44: loss did not improve from 0.00203\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - loss: 0.0022 - val_loss: 5.6698e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m336/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299us/step - loss: 0.0021\n",
      "Epoch 45: loss did not improve from 0.00203\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step - loss: 0.0021 - val_loss: 7.5776e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m319/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317us/step - loss: 0.0021\n",
      "Epoch 46: loss did not improve from 0.00203\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459us/step - loss: 0.0021 - val_loss: 6.7431e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0021\n",
      "Epoch 47: loss did not improve from 0.00203\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - loss: 0.0021 - val_loss: 5.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - loss: 0.0021\n",
      "Epoch 48: loss did not improve from 0.00203\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0021 - val_loss: 8.5118e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m196/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257us/step - loss: 0.0021\n",
      "Epoch 49: loss did not improve from 0.00203\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - loss: 0.0021 - val_loss: 6.7471e-04\n",
      "Epoch 49: early stopping\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "Epoch 1/100\n",
      "\u001b[1m180/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280us/step - loss: 0.1959  \n",
      "Epoch 1: loss improved from inf to 0.05280, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 491us/step - loss: 0.1307 - val_loss: 0.0075\n",
      "Epoch 2/100\n",
      "\u001b[1m276/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step - loss: 0.0160\n",
      "Epoch 2: loss improved from 0.05280 to 0.01445, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 0.0157 - val_loss: 0.0042\n",
      "Epoch 3/100\n",
      "\u001b[1m173/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291us/step - loss: 0.0118\n",
      "Epoch 3: loss improved from 0.01445 to 0.01045, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401us/step - loss: 0.0113 - val_loss: 0.0030\n",
      "Epoch 4/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0086\n",
      "Epoch 4: loss improved from 0.01045 to 0.00773, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344us/step - loss: 0.0083 - val_loss: 0.0025\n",
      "Epoch 5/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0069\n",
      "Epoch 5: loss improved from 0.00773 to 0.00636, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353us/step - loss: 0.0067 - val_loss: 0.0022\n",
      "Epoch 6/100\n",
      "\u001b[1m195/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258us/step - loss: 0.0054\n",
      "Epoch 6: loss improved from 0.00636 to 0.00523, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389us/step - loss: 0.0054 - val_loss: 0.0022\n",
      "Epoch 7/100\n",
      "\u001b[1m216/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468us/step - loss: 0.0049\n",
      "Epoch 7: loss improved from 0.00523 to 0.00475, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - loss: 0.0048 - val_loss: 0.0022\n",
      "Epoch 8/100\n",
      "\u001b[1m358/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281us/step - loss: 0.0045\n",
      "Epoch 8: loss improved from 0.00475 to 0.00441, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390us/step - loss: 0.0045 - val_loss: 0.0019\n",
      "Epoch 9/100\n",
      "\u001b[1m355/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284us/step - loss: 0.0043\n",
      "Epoch 9: loss improved from 0.00441 to 0.00425, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432us/step - loss: 0.0043 - val_loss: 0.0017\n",
      "Epoch 10/100\n",
      "\u001b[1m221/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455us/step - loss: 0.0041\n",
      "Epoch 10: loss improved from 0.00425 to 0.00393, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486us/step - loss: 0.0041 - val_loss: 0.0017\n",
      "Epoch 11/100\n",
      "\u001b[1m190/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265us/step - loss: 0.0039\n",
      "Epoch 11: loss improved from 0.00393 to 0.00381, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358us/step - loss: 0.0039 - val_loss: 0.0016\n",
      "Epoch 12/100\n",
      "\u001b[1m188/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268us/step - loss: 0.0038\n",
      "Epoch 12: loss improved from 0.00381 to 0.00365, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - loss: 0.0037 - val_loss: 0.0019\n",
      "Epoch 13/100\n",
      "\u001b[1m185/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273us/step - loss: 0.0036\n",
      "Epoch 13: loss improved from 0.00365 to 0.00355, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.0036 - val_loss: 0.0014\n",
      "Epoch 14/100\n",
      "\u001b[1m197/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256us/step - loss: 0.0036\n",
      "Epoch 14: loss improved from 0.00355 to 0.00353, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 0.0036 - val_loss: 0.0014\n",
      "Epoch 15/100\n",
      "\u001b[1m184/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273us/step - loss: 0.0036\n",
      "Epoch 15: loss improved from 0.00353 to 0.00350, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step - loss: 0.0036 - val_loss: 0.0014\n",
      "Epoch 16/100\n",
      "\u001b[1m186/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271us/step - loss: 0.0035\n",
      "Epoch 16: loss improved from 0.00350 to 0.00345, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367us/step - loss: 0.0034 - val_loss: 0.0016\n",
      "Epoch 17/100\n",
      "\u001b[1m267/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 0.0033\n",
      "Epoch 17: loss improved from 0.00345 to 0.00336, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 460us/step - loss: 0.0033 - val_loss: 0.0011\n",
      "Epoch 18/100\n",
      "\u001b[1m335/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - loss: 0.0031\n",
      "Epoch 18: loss improved from 0.00336 to 0.00316, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411us/step - loss: 0.0031 - val_loss: 0.0014\n",
      "Epoch 19/100\n",
      "\u001b[1m179/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281us/step - loss: 0.0033\n",
      "Epoch 19: loss did not improve from 0.00316\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344us/step - loss: 0.0033 - val_loss: 0.0015\n",
      "Epoch 20/100\n",
      "\u001b[1m267/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378us/step - loss: 0.0032\n",
      "Epoch 20: loss did not improve from 0.00316\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 0.0032 - val_loss: 0.0011\n",
      "Epoch 21/100\n",
      "\u001b[1m198/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254us/step - loss: 0.0032\n",
      "Epoch 21: loss did not improve from 0.00316\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320us/step - loss: 0.0032 - val_loss: 9.5040e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0032\n",
      "Epoch 22: loss did not improve from 0.00316\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311us/step - loss: 0.0031 - val_loss: 0.0013\n",
      "Epoch 23/100\n",
      "\u001b[1m304/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331us/step - loss: 0.0031\n",
      "Epoch 23: loss improved from 0.00316 to 0.00309, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446us/step - loss: 0.0031 - val_loss: 0.0011\n",
      "Epoch 24/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246us/step - loss: 0.0032\n",
      "Epoch 24: loss improved from 0.00309 to 0.00308, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step - loss: 0.0031 - val_loss: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0032\n",
      "Epoch 25: loss did not improve from 0.00308\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - loss: 0.0031 - val_loss: 8.6697e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0031\n",
      "Epoch 26: loss did not improve from 0.00308\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - loss: 0.0031 - val_loss: 0.0017\n",
      "Epoch 27/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0030\n",
      "Epoch 27: loss improved from 0.00308 to 0.00299, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333us/step - loss: 0.0030 - val_loss: 0.0011\n",
      "Epoch 28/100\n",
      "\u001b[1m172/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293us/step - loss: 0.0030\n",
      "Epoch 28: loss improved from 0.00299 to 0.00297, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357us/step - loss: 0.0030 - val_loss: 0.0012\n",
      "Epoch 29/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0030\n",
      "Epoch 29: loss improved from 0.00297 to 0.00288, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 0.0029 - val_loss: 0.0012\n",
      "Epoch 30/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246us/step - loss: 0.0029\n",
      "Epoch 30: loss did not improve from 0.00288\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309us/step - loss: 0.0029 - val_loss: 9.6010e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m320/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - loss: 0.0029\n",
      "Epoch 31: loss did not improve from 0.00288\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step - loss: 0.0029 - val_loss: 9.6048e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0028\n",
      "Epoch 32: loss improved from 0.00288 to 0.00281, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338us/step - loss: 0.0028 - val_loss: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m178/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284us/step - loss: 0.0028\n",
      "Epoch 33: loss did not improve from 0.00281\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332us/step - loss: 0.0028 - val_loss: 0.0013\n",
      "Epoch 34/100\n",
      "\u001b[1m174/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 0.0027\n",
      "Epoch 34: loss improved from 0.00281 to 0.00272, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - loss: 0.0027 - val_loss: 9.1441e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246us/step - loss: 0.0028\n",
      "Epoch 35: loss did not improve from 0.00272\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - loss: 0.0028 - val_loss: 8.7882e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0026\n",
      "Epoch 36: loss improved from 0.00272 to 0.00270, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372us/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 37/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0027\n",
      "Epoch 37: loss did not improve from 0.00270\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - loss: 0.0027 - val_loss: 9.1048e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m288/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349us/step - loss: 0.0027\n",
      "Epoch 38: loss did not improve from 0.00270\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393us/step - loss: 0.0027 - val_loss: 8.0044e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0027\n",
      "Epoch 39: loss improved from 0.00270 to 0.00267, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334us/step - loss: 0.0027 - val_loss: 8.7544e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m203/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step - loss: 0.0027\n",
      "Epoch 40: loss did not improve from 0.00267\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - loss: 0.0027 - val_loss: 9.9862e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0026\n",
      "Epoch 41: loss improved from 0.00267 to 0.00265, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step - loss: 0.0026 - val_loss: 8.6254e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m217/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 0.0027\n",
      "Epoch 42: loss did not improve from 0.00265\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 610us/step - loss: 0.0027 - val_loss: 8.2505e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244us/step - loss: 0.0026\n",
      "Epoch 43: loss improved from 0.00265 to 0.00264, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333us/step - loss: 0.0026 - val_loss: 9.0772e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m203/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step - loss: 0.0027\n",
      "Epoch 44: loss did not improve from 0.00264\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317us/step - loss: 0.0027 - val_loss: 7.5398e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m323/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - loss: 0.0027\n",
      "Epoch 45: loss improved from 0.00264 to 0.00263, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415us/step - loss: 0.0027 - val_loss: 7.1554e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0027\n",
      "Epoch 46: loss did not improve from 0.00263\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - loss: 0.0027 - val_loss: 9.1385e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0026\n",
      "Epoch 47: loss did not improve from 0.00263\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326us/step - loss: 0.0026 - val_loss: 0.0012\n",
      "Epoch 48/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0027\n",
      "Epoch 48: loss improved from 0.00263 to 0.00262, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333us/step - loss: 0.0027 - val_loss: 6.7326e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246us/step - loss: 0.0027\n",
      "Epoch 49: loss did not improve from 0.00262\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - loss: 0.0027 - val_loss: 8.6390e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0026\n",
      "Epoch 50: loss improved from 0.00262 to 0.00261, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378us/step - loss: 0.0026 - val_loss: 9.4985e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0023\n",
      "Epoch 51: loss improved from 0.00261 to 0.00250, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - loss: 0.0024 - val_loss: 7.3033e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m281/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358us/step - loss: 0.0026\n",
      "Epoch 52: loss did not improve from 0.00250\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398us/step - loss: 0.0026 - val_loss: 0.0010\n",
      "Epoch 53/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0025\n",
      "Epoch 53: loss did not improve from 0.00250\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - loss: 0.0026 - val_loss: 8.7445e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0025\n",
      "Epoch 54: loss did not improve from 0.00250\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - loss: 0.0025 - val_loss: 9.0223e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - loss: 0.0025\n",
      "Epoch 55: loss did not improve from 0.00250\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0025 - val_loss: 8.7979e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m339/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446us/step - loss: 0.0025\n",
      "Epoch 56: loss improved from 0.00250 to 0.00249, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - loss: 0.0025 - val_loss: 0.0010\n",
      "Epoch 57/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0025\n",
      "Epoch 57: loss did not improve from 0.00249\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - loss: 0.0025 - val_loss: 9.0016e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m321/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313us/step - loss: 0.0025\n",
      "Epoch 58: loss did not improve from 0.00249\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step - loss: 0.0025 - val_loss: 7.2158e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - loss: 0.0025\n",
      "Epoch 59: loss did not improve from 0.00249\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0025 - val_loss: 9.4120e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0025\n",
      "Epoch 60: loss improved from 0.00249 to 0.00249, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0025 - val_loss: 6.2621e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m169/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298us/step - loss: 0.0024\n",
      "Epoch 61: loss improved from 0.00249 to 0.00247, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step - loss: 0.0024 - val_loss: 6.6338e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0026\n",
      "Epoch 62: loss did not improve from 0.00247\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - loss: 0.0026 - val_loss: 5.9685e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0025\n",
      "Epoch 63: loss did not improve from 0.00247\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310us/step - loss: 0.0025 - val_loss: 6.6336e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m286/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 0.0024\n",
      "Epoch 64: loss improved from 0.00247 to 0.00243, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - loss: 0.0024 - val_loss: 9.9077e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0025\n",
      "Epoch 65: loss did not improve from 0.00243\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327us/step - loss: 0.0025 - val_loss: 6.2941e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0026\n",
      "Epoch 66: loss did not improve from 0.00243\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0025 - val_loss: 9.5954e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0025\n",
      "Epoch 67: loss did not improve from 0.00243\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301us/step - loss: 0.0025 - val_loss: 8.7313e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0024\n",
      "Epoch 68: loss did not improve from 0.00243\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0024 - val_loss: 7.0551e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m192/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262us/step - loss: 0.0025\n",
      "Epoch 69: loss did not improve from 0.00243\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315us/step - loss: 0.0025 - val_loss: 6.6894e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m214/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235us/step - loss: 0.0025\n",
      "Epoch 70: loss did not improve from 0.00243\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296us/step - loss: 0.0025 - val_loss: 7.4355e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m304/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333us/step - loss: 0.0024\n",
      "Epoch 71: loss did not improve from 0.00243\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.0024 - val_loss: 5.8097e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - loss: 0.0024\n",
      "Epoch 72: loss did not improve from 0.00243\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301us/step - loss: 0.0024 - val_loss: 8.7349e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0024\n",
      "Epoch 73: loss did not improve from 0.00243\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301us/step - loss: 0.0025 - val_loss: 6.6070e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0025\n",
      "Epoch 74: loss did not improve from 0.00243\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - loss: 0.0025 - val_loss: 0.0010\n",
      "Epoch 74: early stopping\n",
      "Restoring model weights from the end of the best epoch: 64.\n",
      "Epoch 1/100\n",
      "\u001b[1m172/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294us/step - loss: 0.0620  \n",
      "Epoch 1: loss improved from inf to 0.02456, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 491us/step - loss: 0.0446 - val_loss: 0.0058\n",
      "Epoch 2/100\n",
      "\u001b[1m327/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - loss: 0.0118\n",
      "Epoch 2: loss improved from 0.02456 to 0.01034, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.0117 - val_loss: 0.0038\n",
      "Epoch 3/100\n",
      "\u001b[1m304/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331us/step - loss: 0.0079\n",
      "Epoch 3: loss improved from 0.01034 to 0.00738, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455us/step - loss: 0.0079 - val_loss: 0.0027\n",
      "Epoch 4/100\n",
      "\u001b[1m202/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249us/step - loss: 0.0063\n",
      "Epoch 4: loss improved from 0.00738 to 0.00580, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362us/step - loss: 0.0062 - val_loss: 0.0025\n",
      "Epoch 5/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246us/step - loss: 0.0053\n",
      "Epoch 5: loss improved from 0.00580 to 0.00488, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step - loss: 0.0051 - val_loss: 0.0022\n",
      "Epoch 6/100\n",
      "\u001b[1m202/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249us/step - loss: 0.0044\n",
      "Epoch 6: loss improved from 0.00488 to 0.00439, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0044 - val_loss: 0.0018\n",
      "Epoch 7/100\n",
      "\u001b[1m203/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step - loss: 0.0041\n",
      "Epoch 7: loss improved from 0.00439 to 0.00397, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - loss: 0.0040 - val_loss: 0.0018\n",
      "Epoch 8/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0038\n",
      "Epoch 8: loss improved from 0.00397 to 0.00379, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 428us/step - loss: 0.0038 - val_loss: 0.0016\n",
      "Epoch 9/100\n",
      "\u001b[1m161/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 0.0036\n",
      "Epoch 9: loss improved from 0.00379 to 0.00352, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 0.0036 - val_loss: 0.0017\n",
      "Epoch 10/100\n",
      "\u001b[1m333/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0034\n",
      "Epoch 10: loss improved from 0.00352 to 0.00342, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444us/step - loss: 0.0034 - val_loss: 0.0014\n",
      "Epoch 11/100\n",
      "\u001b[1m165/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - loss: 0.0031\n",
      "Epoch 11: loss improved from 0.00342 to 0.00327, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379us/step - loss: 0.0032 - val_loss: 0.0016\n",
      "Epoch 12/100\n",
      "\u001b[1m350/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287us/step - loss: 0.0032\n",
      "Epoch 12: loss improved from 0.00327 to 0.00313, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412us/step - loss: 0.0032 - val_loss: 0.0015\n",
      "Epoch 13/100\n",
      "\u001b[1m176/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287us/step - loss: 0.0029\n",
      "Epoch 13: loss improved from 0.00313 to 0.00299, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364us/step - loss: 0.0029 - val_loss: 0.0013\n",
      "Epoch 14/100\n",
      "\u001b[1m280/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362us/step - loss: 0.0030\n",
      "Epoch 14: loss did not improve from 0.00299\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405us/step - loss: 0.0031 - val_loss: 0.0011\n",
      "Epoch 15/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - loss: 0.0030\n",
      "Epoch 15: loss improved from 0.00299 to 0.00291, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335us/step - loss: 0.0030 - val_loss: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m203/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step - loss: 0.0030\n",
      "Epoch 16: loss did not improve from 0.00291\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311us/step - loss: 0.0030 - val_loss: 0.0012\n",
      "Epoch 17/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0030\n",
      "Epoch 17: loss improved from 0.00291 to 0.00289, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378us/step - loss: 0.0030 - val_loss: 0.0011\n",
      "Epoch 18/100\n",
      "\u001b[1m172/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294us/step - loss: 0.0028\n",
      "Epoch 18: loss improved from 0.00289 to 0.00283, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - loss: 0.0028 - val_loss: 9.8260e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0027\n",
      "Epoch 19: loss improved from 0.00283 to 0.00280, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.0028 - val_loss: 0.0011\n",
      "Epoch 20/100\n",
      "\u001b[1m192/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263us/step - loss: 0.0027\n",
      "Epoch 20: loss improved from 0.00280 to 0.00272, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393us/step - loss: 0.0027 - val_loss: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m283/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 0.0027\n",
      "Epoch 21: loss improved from 0.00272 to 0.00267, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.0027 - val_loss: 9.8336e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m203/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step - loss: 0.0027\n",
      "Epoch 22: loss did not improve from 0.00267\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - loss: 0.0027 - val_loss: 7.9984e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m248/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416us/step - loss: 0.0026\n",
      "Epoch 23: loss improved from 0.00267 to 0.00264, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - loss: 0.0026 - val_loss: 9.2820e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m202/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250us/step - loss: 0.0027\n",
      "Epoch 24: loss improved from 0.00264 to 0.00262, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341us/step - loss: 0.0026 - val_loss: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m202/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249us/step - loss: 0.0026\n",
      "Epoch 25: loss improved from 0.00262 to 0.00256, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step - loss: 0.0025 - val_loss: 7.9469e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m333/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - loss: 0.0026\n",
      "Epoch 26: loss did not improve from 0.00256\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398us/step - loss: 0.0026 - val_loss: 8.9271e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246us/step - loss: 0.0026\n",
      "Epoch 27: loss improved from 0.00256 to 0.00256, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step - loss: 0.0026 - val_loss: 8.9218e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m316/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482us/step - loss: 0.0025\n",
      "Epoch 28: loss improved from 0.00256 to 0.00251, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - loss: 0.0025 - val_loss: 8.5699e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0026\n",
      "Epoch 29: loss did not improve from 0.00251\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311us/step - loss: 0.0026 - val_loss: 9.2225e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0026\n",
      "Epoch 30: loss did not improve from 0.00251\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0026 - val_loss: 9.7534e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0026\n",
      "Epoch 31: loss improved from 0.00251 to 0.00251, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359us/step - loss: 0.0025 - val_loss: 9.5076e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m286/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - loss: 0.0025\n",
      "Epoch 32: loss improved from 0.00251 to 0.00248, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424us/step - loss: 0.0025 - val_loss: 8.9192e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m203/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step - loss: 0.0024\n",
      "Epoch 33: loss did not improve from 0.00248\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - loss: 0.0024 - val_loss: 7.1496e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0025\n",
      "Epoch 34: loss improved from 0.00248 to 0.00247, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - loss: 0.0025 - val_loss: 8.2636e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244us/step - loss: 0.0026\n",
      "Epoch 35: loss improved from 0.00247 to 0.00243, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334us/step - loss: 0.0025 - val_loss: 6.8774e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0025\n",
      "Epoch 36: loss did not improve from 0.00243\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 37/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0025\n",
      "Epoch 37: loss did not improve from 0.00243\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - loss: 0.0025 - val_loss: 9.3585e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0025\n",
      "Epoch 38: loss did not improve from 0.00243\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - loss: 0.0025 - val_loss: 6.6537e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m319/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315us/step - loss: 0.0024\n",
      "Epoch 39: loss improved from 0.00243 to 0.00242, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401us/step - loss: 0.0024 - val_loss: 7.1092e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m174/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290us/step - loss: 0.0024\n",
      "Epoch 40: loss did not improve from 0.00242\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327us/step - loss: 0.0024 - val_loss: 8.3087e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - loss: 0.0024\n",
      "Epoch 41: loss did not improve from 0.00242\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - loss: 0.0024 - val_loss: 6.0282e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m357/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425us/step - loss: 0.0024\n",
      "Epoch 42: loss did not improve from 0.00242\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497us/step - loss: 0.0024 - val_loss: 9.9882e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m181/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279us/step - loss: 0.0024\n",
      "Epoch 43: loss did not improve from 0.00242\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334us/step - loss: 0.0025 - val_loss: 8.6413e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m310/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327us/step - loss: 0.0024\n",
      "Epoch 44: loss did not improve from 0.00242\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408us/step - loss: 0.0024 - val_loss: 5.9524e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244us/step - loss: 0.0024\n",
      "Epoch 45: loss did not improve from 0.00242\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - loss: 0.0024 - val_loss: 6.0397e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0025\n",
      "Epoch 46: loss improved from 0.00242 to 0.00238, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 0.0024 - val_loss: 7.9152e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m200/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252us/step - loss: 0.0024\n",
      "Epoch 47: loss did not improve from 0.00238\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311us/step - loss: 0.0024 - val_loss: 6.7566e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244us/step - loss: 0.0023\n",
      "Epoch 48: loss improved from 0.00238 to 0.00238, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401us/step - loss: 0.0023 - val_loss: 6.4792e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0024\n",
      "Epoch 49: loss improved from 0.00238 to 0.00237, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - loss: 0.0024 - val_loss: 9.8172e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m181/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279us/step - loss: 0.0024\n",
      "Epoch 50: loss did not improve from 0.00237\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step - loss: 0.0024 - val_loss: 5.3174e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m203/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step - loss: 0.0024\n",
      "Epoch 51: loss did not improve from 0.00237\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319us/step - loss: 0.0024 - val_loss: 6.7819e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0023\n",
      "Epoch 52: loss did not improve from 0.00237\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328us/step - loss: 0.0023 - val_loss: 8.9639e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0025\n",
      "Epoch 53: loss did not improve from 0.00237\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0024 - val_loss: 7.1363e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0023\n",
      "Epoch 54: loss did not improve from 0.00237\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - loss: 0.0023 - val_loss: 6.4518e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0025\n",
      "Epoch 55: loss did not improve from 0.00237\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303us/step - loss: 0.0024 - val_loss: 5.9789e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m330/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - loss: 0.0024\n",
      "Epoch 56: loss did not improve from 0.00237\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step - loss: 0.0024 - val_loss: 6.6986e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0024\n",
      "Epoch 57: loss did not improve from 0.00237\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321us/step - loss: 0.0024 - val_loss: 7.4113e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0024\n",
      "Epoch 58: loss did not improve from 0.00237\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299us/step - loss: 0.0024 - val_loss: 5.5930e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0023\n",
      "Epoch 59: loss did not improve from 0.00237\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309us/step - loss: 0.0024 - val_loss: 7.8494e-04\n",
      "Epoch 59: early stopping\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "Epoch 1/100\n",
      "\u001b[1m183/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275us/step - loss: 0.1140  \n",
      "Epoch 1: loss improved from inf to 0.03317, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step - loss: 0.0778 - val_loss: 0.0060\n",
      "Epoch 2/100\n",
      "\u001b[1m266/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378us/step - loss: 0.0122\n",
      "Epoch 2: loss improved from 0.03317 to 0.01091, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 0.0119 - val_loss: 0.0034\n",
      "Epoch 3/100\n",
      "\u001b[1m203/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step - loss: 0.0084\n",
      "Epoch 3: loss improved from 0.01091 to 0.00806, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372us/step - loss: 0.0083 - val_loss: 0.0032\n",
      "Epoch 4/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0071\n",
      "Epoch 4: loss improved from 0.00806 to 0.00636, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336us/step - loss: 0.0069 - val_loss: 0.0024\n",
      "Epoch 5/100\n",
      "\u001b[1m353/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285us/step - loss: 0.0054\n",
      "Epoch 5: loss improved from 0.00636 to 0.00529, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - loss: 0.0054 - val_loss: 0.0026\n",
      "Epoch 6/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0048\n",
      "Epoch 6: loss improved from 0.00529 to 0.00464, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338us/step - loss: 0.0047 - val_loss: 0.0021\n",
      "Epoch 7/100\n",
      "\u001b[1m318/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316us/step - loss: 0.0043\n",
      "Epoch 7: loss improved from 0.00464 to 0.00412, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405us/step - loss: 0.0043 - val_loss: 0.0025\n",
      "Epoch 8/100\n",
      "\u001b[1m199/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253us/step - loss: 0.0040\n",
      "Epoch 8: loss improved from 0.00412 to 0.00393, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382us/step - loss: 0.0040 - val_loss: 0.0018\n",
      "Epoch 9/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244us/step - loss: 0.0038\n",
      "Epoch 9: loss improved from 0.00393 to 0.00378, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - loss: 0.0038 - val_loss: 0.0015\n",
      "Epoch 10/100\n",
      "\u001b[1m193/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261us/step - loss: 0.0037\n",
      "Epoch 10: loss improved from 0.00378 to 0.00362, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349us/step - loss: 0.0037 - val_loss: 0.0016\n",
      "Epoch 11/100\n",
      "\u001b[1m200/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252us/step - loss: 0.0036\n",
      "Epoch 11: loss improved from 0.00362 to 0.00342, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 12/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0035\n",
      "Epoch 12: loss did not improve from 0.00342\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - loss: 0.0035 - val_loss: 0.0014\n",
      "Epoch 13/100\n",
      "\u001b[1m326/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - loss: 0.0034\n",
      "Epoch 13: loss improved from 0.00342 to 0.00333, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396us/step - loss: 0.0034 - val_loss: 0.0013\n",
      "Epoch 14/100\n",
      "\u001b[1m203/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step - loss: 0.0032\n",
      "Epoch 14: loss improved from 0.00333 to 0.00309, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step - loss: 0.0031 - val_loss: 0.0014\n",
      "Epoch 15/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0030\n",
      "Epoch 15: loss improved from 0.00309 to 0.00302, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step - loss: 0.0030 - val_loss: 0.0014\n",
      "Epoch 16/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0030\n",
      "Epoch 16: loss improved from 0.00302 to 0.00298, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338us/step - loss: 0.0030 - val_loss: 0.0014\n",
      "Epoch 17/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0030\n",
      "Epoch 17: loss improved from 0.00298 to 0.00296, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338us/step - loss: 0.0030 - val_loss: 0.0012\n",
      "Epoch 18/100\n",
      "\u001b[1m266/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409us/step - loss: 0.0028\n",
      "Epoch 18: loss improved from 0.00296 to 0.00286, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476us/step - loss: 0.0028 - val_loss: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0028\n",
      "Epoch 19: loss improved from 0.00286 to 0.00279, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340us/step - loss: 0.0028 - val_loss: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m202/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249us/step - loss: 0.0028\n",
      "Epoch 20: loss improved from 0.00279 to 0.00274, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step - loss: 0.0028 - val_loss: 8.4718e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m196/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258us/step - loss: 0.0027\n",
      "Epoch 21: loss improved from 0.00274 to 0.00270, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379us/step - loss: 0.0027 - val_loss: 0.0011\n",
      "Epoch 22/100\n",
      "\u001b[1m339/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474us/step - loss: 0.0027\n",
      "Epoch 22: loss improved from 0.00270 to 0.00261, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.0027 - val_loss: 9.1074e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0027\n",
      "Epoch 23: loss did not improve from 0.00261\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - loss: 0.0027 - val_loss: 0.0011\n",
      "Epoch 24/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0026\n",
      "Epoch 24: loss did not improve from 0.00261\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - loss: 0.0026 - val_loss: 9.2807e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step - loss: 0.0026\n",
      "Epoch 25: loss did not improve from 0.00261\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - loss: 0.0026 - val_loss: 0.0011\n",
      "Epoch 26/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0026\n",
      "Epoch 26: loss did not improve from 0.00261\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - loss: 0.0026 - val_loss: 7.4813e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m291/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 0.0027\n",
      "Epoch 27: loss did not improve from 0.00261\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392us/step - loss: 0.0026 - val_loss: 9.6678e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0026\n",
      "Epoch 28: loss improved from 0.00261 to 0.00261, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step - loss: 0.0026 - val_loss: 9.7366e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0025\n",
      "Epoch 29: loss improved from 0.00261 to 0.00254, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - loss: 0.0025 - val_loss: 8.7610e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m339/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447us/step - loss: 0.0025\n",
      "Epoch 30: loss improved from 0.00254 to 0.00254, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - loss: 0.0025 - val_loss: 7.2609e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0027\n",
      "Epoch 31: loss did not improve from 0.00254\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - loss: 0.0026 - val_loss: 7.7168e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0026\n",
      "Epoch 32: loss did not improve from 0.00254\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303us/step - loss: 0.0026 - val_loss: 8.2341e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m214/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step - loss: 0.0025\n",
      "Epoch 33: loss improved from 0.00254 to 0.00250, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329us/step - loss: 0.0025 - val_loss: 9.5680e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m190/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265us/step - loss: 0.0025\n",
      "Epoch 34: loss did not improve from 0.00250\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316us/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 35/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0025\n",
      "Epoch 35: loss improved from 0.00250 to 0.00249, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349us/step - loss: 0.0025 - val_loss: 6.0842e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - loss: 0.0026\n",
      "Epoch 36: loss improved from 0.00249 to 0.00248, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335us/step - loss: 0.0025 - val_loss: 8.8093e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m173/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291us/step - loss: 0.0024\n",
      "Epoch 37: loss improved from 0.00248 to 0.00247, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383us/step - loss: 0.0025 - val_loss: 8.3770e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0024\n",
      "Epoch 38: loss did not improve from 0.00247\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - loss: 0.0024 - val_loss: 7.9348e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m269/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - loss: 0.0024\n",
      "Epoch 39: loss improved from 0.00247 to 0.00243, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434us/step - loss: 0.0024 - val_loss: 9.3627e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244us/step - loss: 0.0025\n",
      "Epoch 40: loss did not improve from 0.00243\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326us/step - loss: 0.0025 - val_loss: 9.1142e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0023\n",
      "Epoch 41: loss improved from 0.00243 to 0.00237, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360us/step - loss: 0.0024 - val_loss: 0.0012\n",
      "Epoch 42/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0024\n",
      "Epoch 42: loss did not improve from 0.00237\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311us/step - loss: 0.0024 - val_loss: 6.8812e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m200/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252us/step - loss: 0.0022\n",
      "Epoch 43: loss improved from 0.00237 to 0.00231, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406us/step - loss: 0.0023 - val_loss: 6.9530e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m252/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - loss: 0.0023\n",
      "Epoch 44: loss did not improve from 0.00231\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 0.0023 - val_loss: 7.1663e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m201/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251us/step - loss: 0.0025\n",
      "Epoch 45: loss did not improve from 0.00231\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318us/step - loss: 0.0025 - val_loss: 9.6304e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0024\n",
      "Epoch 46: loss did not improve from 0.00231\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - loss: 0.0023 - val_loss: 6.7477e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0023\n",
      "Epoch 47: loss did not improve from 0.00231\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.0023 - val_loss: 8.6281e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0024\n",
      "Epoch 48: loss did not improve from 0.00231\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - loss: 0.0024 - val_loss: 7.4627e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m312/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323us/step - loss: 0.0024\n",
      "Epoch 49: loss did not improve from 0.00231\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 0.0024 - val_loss: 8.2323e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - loss: 0.0023\n",
      "Epoch 50: loss did not improve from 0.00231\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 51/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0024\n",
      "Epoch 51: loss did not improve from 0.00231\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step - loss: 0.0024 - val_loss: 6.3578e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490us/step - loss: 0.0024\n",
      "Epoch 52: loss did not improve from 0.00231\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 449us/step - loss: 0.0023 - val_loss: 8.9579e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0023\n",
      "Epoch 53: loss did not improve from 0.00231\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311us/step - loss: 0.0023 - val_loss: 6.4684e-04\n",
      "Epoch 53: early stopping\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "Epoch 1/100\n",
      "\u001b[1m351/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287us/step - loss: 0.2029 \n",
      "Epoch 1: loss improved from inf to 0.07321, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 0.1997 - val_loss: 0.0078\n",
      "Epoch 2/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0179\n",
      "Epoch 2: loss improved from 0.07321 to 0.01572, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step - loss: 0.0173 - val_loss: 0.0043\n",
      "Epoch 3/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - loss: 0.0130\n",
      "Epoch 3: loss improved from 0.01572 to 0.01143, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389us/step - loss: 0.0125 - val_loss: 0.0036\n",
      "Epoch 4/100\n",
      "\u001b[1m278/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step - loss: 0.0092\n",
      "Epoch 4: loss improved from 0.01143 to 0.00868, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 433us/step - loss: 0.0091 - val_loss: 0.0027\n",
      "Epoch 5/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246us/step - loss: 0.0073\n",
      "Epoch 5: loss improved from 0.00868 to 0.00705, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359us/step - loss: 0.0072 - val_loss: 0.0023\n",
      "Epoch 6/100\n",
      "\u001b[1m351/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286us/step - loss: 0.0061\n",
      "Epoch 6: loss improved from 0.00705 to 0.00596, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394us/step - loss: 0.0061 - val_loss: 0.0021\n",
      "Epoch 7/100\n",
      "\u001b[1m190/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265us/step - loss: 0.0052\n",
      "Epoch 7: loss improved from 0.00596 to 0.00507, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.0052 - val_loss: 0.0020\n",
      "Epoch 8/100\n",
      "\u001b[1m282/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358us/step - loss: 0.0048\n",
      "Epoch 8: loss improved from 0.00507 to 0.00455, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476us/step - loss: 0.0048 - val_loss: 0.0023\n",
      "Epoch 9/100\n",
      "\u001b[1m237/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - loss: 0.0042\n",
      "Epoch 9: loss improved from 0.00455 to 0.00414, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - loss: 0.0042 - val_loss: 0.0021\n",
      "Epoch 10/100\n",
      "\u001b[1m320/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - loss: 0.0038\n",
      "Epoch 10: loss improved from 0.00414 to 0.00377, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416us/step - loss: 0.0038 - val_loss: 0.0017\n",
      "Epoch 11/100\n",
      "\u001b[1m194/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259us/step - loss: 0.0036\n",
      "Epoch 11: loss improved from 0.00377 to 0.00356, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372us/step - loss: 0.0036 - val_loss: 0.0018\n",
      "Epoch 12/100\n",
      "\u001b[1m242/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420us/step - loss: 0.0035\n",
      "Epoch 12: loss improved from 0.00356 to 0.00337, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512us/step - loss: 0.0034 - val_loss: 0.0014\n",
      "Epoch 13/100\n",
      "\u001b[1m181/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278us/step - loss: 0.0034\n",
      "Epoch 13: loss improved from 0.00337 to 0.00328, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step - loss: 0.0034 - val_loss: 0.0018\n",
      "Epoch 14/100\n",
      "\u001b[1m192/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262us/step - loss: 0.0031\n",
      "Epoch 14: loss improved from 0.00328 to 0.00309, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - loss: 0.0031 - val_loss: 0.0013\n",
      "Epoch 15/100\n",
      "\u001b[1m251/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405us/step - loss: 0.0030\n",
      "Epoch 15: loss improved from 0.00309 to 0.00302, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482us/step - loss: 0.0030 - val_loss: 0.0011\n",
      "Epoch 16/100\n",
      "\u001b[1m351/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286us/step - loss: 0.0030\n",
      "Epoch 16: loss improved from 0.00302 to 0.00299, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424us/step - loss: 0.0030 - val_loss: 0.0014\n",
      "Epoch 17/100\n",
      "\u001b[1m171/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295us/step - loss: 0.0028\n",
      "Epoch 17: loss improved from 0.00299 to 0.00290, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386us/step - loss: 0.0029 - val_loss: 0.0012\n",
      "Epoch 18/100\n",
      "\u001b[1m260/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.0029\n",
      "Epoch 18: loss improved from 0.00290 to 0.00283, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - loss: 0.0029 - val_loss: 0.0011\n",
      "Epoch 19/100\n",
      "\u001b[1m349/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288us/step - loss: 0.0027\n",
      "Epoch 19: loss improved from 0.00283 to 0.00277, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383us/step - loss: 0.0027 - val_loss: 9.5005e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m201/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250us/step - loss: 0.0027\n",
      "Epoch 20: loss improved from 0.00277 to 0.00274, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439us/step - loss: 0.0028 - val_loss: 0.0014\n",
      "Epoch 21/100\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280us/step - loss: 0.0027\n",
      "Epoch 21: loss improved from 0.00274 to 0.00270, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422us/step - loss: 0.0027 - val_loss: 8.6660e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m351/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286us/step - loss: 0.0026\n",
      "Epoch 22: loss improved from 0.00270 to 0.00264, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 430us/step - loss: 0.0026 - val_loss: 0.0011\n",
      "Epoch 23/100\n",
      "\u001b[1m353/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285us/step - loss: 0.0027\n",
      "Epoch 23: loss did not improve from 0.00264\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410us/step - loss: 0.0027 - val_loss: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m313/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321us/step - loss: 0.0026\n",
      "Epoch 24: loss improved from 0.00264 to 0.00262, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 429us/step - loss: 0.0026 - val_loss: 8.8774e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m200/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - loss: 0.0025\n",
      "Epoch 25: loss improved from 0.00262 to 0.00260, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - loss: 0.0025 - val_loss: 0.0012\n",
      "Epoch 26/100\n",
      "\u001b[1m343/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - loss: 0.0026\n",
      "Epoch 26: loss improved from 0.00260 to 0.00249, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 0.0026 - val_loss: 8.8345e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m174/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289us/step - loss: 0.0025\n",
      "Epoch 27: loss improved from 0.00249 to 0.00246, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448us/step - loss: 0.0025 - val_loss: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m181/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279us/step - loss: 0.0024\n",
      "Epoch 28: loss improved from 0.00246 to 0.00244, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385us/step - loss: 0.0024 - val_loss: 8.6777e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m183/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276us/step - loss: 0.0025\n",
      "Epoch 29: loss did not improve from 0.00244\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423us/step - loss: 0.0025 - val_loss: 8.8505e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m336/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - loss: 0.0025\n",
      "Epoch 30: loss did not improve from 0.00244\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step - loss: 0.0025 - val_loss: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m344/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296us/step - loss: 0.0024\n",
      "Epoch 31: loss improved from 0.00244 to 0.00241, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 0.0024 - val_loss: 9.2521e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m352/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286us/step - loss: 0.0024\n",
      "Epoch 32: loss improved from 0.00241 to 0.00241, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441us/step - loss: 0.0024 - val_loss: 8.0868e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m298/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step - loss: 0.0024\n",
      "Epoch 33: loss improved from 0.00241 to 0.00235, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455us/step - loss: 0.0024 - val_loss: 7.1033e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0024\n",
      "Epoch 34: loss did not improve from 0.00235\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - loss: 0.0024 - val_loss: 9.7134e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m177/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284us/step - loss: 0.0024\n",
      "Epoch 35: loss did not improve from 0.00235\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321us/step - loss: 0.0024 - val_loss: 0.0011\n",
      "Epoch 36/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246us/step - loss: 0.0024\n",
      "Epoch 36: loss improved from 0.00235 to 0.00235, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - loss: 0.0024 - val_loss: 8.5102e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0023\n",
      "Epoch 37: loss did not improve from 0.00235\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325us/step - loss: 0.0023 - val_loss: 0.0011\n",
      "Epoch 38/100\n",
      "\u001b[1m197/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255us/step - loss: 0.0024\n",
      "Epoch 38: loss did not improve from 0.00235\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332us/step - loss: 0.0024 - val_loss: 7.0628e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step - loss: 0.0023\n",
      "Epoch 39: loss improved from 0.00235 to 0.00230, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331us/step - loss: 0.0023 - val_loss: 7.8172e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0024\n",
      "Epoch 40: loss did not improve from 0.00230\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298us/step - loss: 0.0024 - val_loss: 7.1814e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m215/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234us/step - loss: 0.0023\n",
      "Epoch 41: loss did not improve from 0.00230\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295us/step - loss: 0.0023 - val_loss: 6.1361e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m308/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327us/step - loss: 0.0023\n",
      "Epoch 42: loss improved from 0.00230 to 0.00230, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415us/step - loss: 0.0023 - val_loss: 6.2198e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m194/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260us/step - loss: 0.0024\n",
      "Epoch 43: loss did not improve from 0.00230\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318us/step - loss: 0.0024 - val_loss: 8.7157e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0022\n",
      "Epoch 44: loss improved from 0.00230 to 0.00225, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391us/step - loss: 0.0022 - val_loss: 6.8042e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m288/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349us/step - loss: 0.0023\n",
      "Epoch 45: loss did not improve from 0.00225\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393us/step - loss: 0.0023 - val_loss: 7.1522e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m213/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0022\n",
      "Epoch 46: loss did not improve from 0.00225\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362us/step - loss: 0.0022 - val_loss: 6.6782e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0023\n",
      "Epoch 47: loss did not improve from 0.00225\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0023 - val_loss: 7.5777e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m353/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285us/step - loss: 0.0023\n",
      "Epoch 48: loss improved from 0.00225 to 0.00223, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423us/step - loss: 0.0023 - val_loss: 6.0240e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - loss: 0.0023\n",
      "Epoch 49: loss did not improve from 0.00223\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0023 - val_loss: 6.5273e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0023\n",
      "Epoch 50: loss improved from 0.00223 to 0.00220, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411us/step - loss: 0.0023 - val_loss: 5.5774e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m174/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290us/step - loss: 0.0023\n",
      "Epoch 51: loss did not improve from 0.00220\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331us/step - loss: 0.0023 - val_loss: 6.7559e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m318/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 477us/step - loss: 0.0022\n",
      "Epoch 52: loss did not improve from 0.00220\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - loss: 0.0022 - val_loss: 6.5625e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m201/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251us/step - loss: 0.0022\n",
      "Epoch 53: loss did not improve from 0.00220\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316us/step - loss: 0.0022 - val_loss: 7.5924e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m322/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step - loss: 0.0022\n",
      "Epoch 54: loss did not improve from 0.00220\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403us/step - loss: 0.0022 - val_loss: 6.0210e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m193/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261us/step - loss: 0.0022\n",
      "Epoch 55: loss did not improve from 0.00220\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318us/step - loss: 0.0022 - val_loss: 7.7788e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m203/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step - loss: 0.0022\n",
      "Epoch 56: loss did not improve from 0.00220\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step - loss: 0.0022 - val_loss: 6.7462e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0022\n",
      "Epoch 57: loss did not improve from 0.00220\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328us/step - loss: 0.0022 - val_loss: 7.1608e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0022\n",
      "Epoch 58: loss did not improve from 0.00220\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0022 - val_loss: 5.7413e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m320/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - loss: 0.0021\n",
      "Epoch 59: loss improved from 0.00220 to 0.00217, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403us/step - loss: 0.0021 - val_loss: 7.7875e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - loss: 0.0021\n",
      "Epoch 60: loss improved from 0.00217 to 0.00214, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391us/step - loss: 0.0021 - val_loss: 6.7112e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0023\n",
      "Epoch 61: loss did not improve from 0.00214\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - loss: 0.0023 - val_loss: 6.3718e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m320/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - loss: 0.0022\n",
      "Epoch 62: loss did not improve from 0.00214\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382us/step - loss: 0.0022 - val_loss: 0.0010\n",
      "Epoch 63/100\n",
      "\u001b[1m168/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - loss: 0.0020\n",
      "Epoch 63: loss improved from 0.00214 to 0.00213, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - loss: 0.0021 - val_loss: 9.2602e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0021\n",
      "Epoch 64: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301us/step - loss: 0.0021 - val_loss: 5.1823e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m332/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.0022\n",
      "Epoch 65: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391us/step - loss: 0.0022 - val_loss: 0.0011\n",
      "Epoch 66/100\n",
      "\u001b[1m323/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - loss: 0.0022\n",
      "Epoch 66: loss improved from 0.00213 to 0.00213, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.0022 - val_loss: 5.8196e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m332/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - loss: 0.0022\n",
      "Epoch 67: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406us/step - loss: 0.0022 - val_loss: 7.9766e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m196/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257us/step - loss: 0.0021\n",
      "Epoch 68: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320us/step - loss: 0.0021 - val_loss: 5.6623e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0022\n",
      "Epoch 69: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - loss: 0.0022 - val_loss: 7.7117e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m306/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328us/step - loss: 0.0022\n",
      "Epoch 70: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - loss: 0.0022 - val_loss: 9.5091e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m177/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284us/step - loss: 0.0021\n",
      "Epoch 71: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 0.0021 - val_loss: 6.7734e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - loss: 0.0021\n",
      "Epoch 72: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - loss: 0.0021 - val_loss: 5.4479e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m236/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 0.0022\n",
      "Epoch 73: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.0022 - val_loss: 5.8111e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m181/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278us/step - loss: 0.0023\n",
      "Epoch 74: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326us/step - loss: 0.0022 - val_loss: 8.0386e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0021\n",
      "Epoch 75: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301us/step - loss: 0.0021 - val_loss: 7.3053e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m289/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353us/step - loss: 0.0022\n",
      "Epoch 76: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395us/step - loss: 0.0022 - val_loss: 6.9743e-04\n",
      "Epoch 76: early stopping\n",
      "Restoring model weights from the end of the best epoch: 66.\n",
      "Epoch 1/100\n",
      "\u001b[1m174/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289us/step - loss: 0.0557  \n",
      "Epoch 1: loss improved from inf to 0.02295, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485us/step - loss: 0.0410 - val_loss: 0.0046\n",
      "Epoch 2/100\n",
      "\u001b[1m285/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - loss: 0.0103\n",
      "Epoch 2: loss improved from 0.02295 to 0.00900, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471us/step - loss: 0.0101 - val_loss: 0.0025\n",
      "Epoch 3/100\n",
      "\u001b[1m177/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285us/step - loss: 0.0073\n",
      "Epoch 3: loss improved from 0.00900 to 0.00656, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394us/step - loss: 0.0070 - val_loss: 0.0029\n",
      "Epoch 4/100\n",
      "\u001b[1m186/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270us/step - loss: 0.0056\n",
      "Epoch 4: loss improved from 0.00656 to 0.00524, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372us/step - loss: 0.0055 - val_loss: 0.0020\n",
      "Epoch 5/100\n",
      "\u001b[1m310/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325us/step - loss: 0.0046\n",
      "Epoch 5: loss improved from 0.00524 to 0.00456, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409us/step - loss: 0.0046 - val_loss: 0.0019\n",
      "Epoch 6/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246us/step - loss: 0.0041\n",
      "Epoch 6: loss improved from 0.00456 to 0.00406, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0041 - val_loss: 0.0016\n",
      "Epoch 7/100\n",
      "\u001b[1m198/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254us/step - loss: 0.0037\n",
      "Epoch 7: loss improved from 0.00406 to 0.00368, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342us/step - loss: 0.0037 - val_loss: 0.0017\n",
      "Epoch 8/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0039\n",
      "Epoch 8: loss improved from 0.00368 to 0.00359, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 0.0038 - val_loss: 0.0014\n",
      "Epoch 9/100\n",
      "\u001b[1m202/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249us/step - loss: 0.0035\n",
      "Epoch 9: loss improved from 0.00359 to 0.00337, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340us/step - loss: 0.0034 - val_loss: 0.0017\n",
      "Epoch 10/100\n",
      "\u001b[1m172/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293us/step - loss: 0.0033\n",
      "Epoch 10: loss improved from 0.00337 to 0.00330, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 11/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0032\n",
      "Epoch 11: loss improved from 0.00330 to 0.00316, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - loss: 0.0032 - val_loss: 0.0015\n",
      "Epoch 12/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0032\n",
      "Epoch 12: loss improved from 0.00316 to 0.00315, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step - loss: 0.0032 - val_loss: 0.0014\n",
      "Epoch 13/100\n",
      "\u001b[1m152/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332us/step - loss: 0.0032\n",
      "Epoch 13: loss improved from 0.00315 to 0.00311, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397us/step - loss: 0.0032 - val_loss: 0.0015\n",
      "Epoch 14/100\n",
      "\u001b[1m215/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235us/step - loss: 0.0030\n",
      "Epoch 14: loss improved from 0.00311 to 0.00297, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344us/step - loss: 0.0030 - val_loss: 0.0011\n",
      "Epoch 15/100\n",
      "\u001b[1m256/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - loss: 0.0030\n",
      "Epoch 15: loss did not improve from 0.00297\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - loss: 0.0030 - val_loss: 0.0012\n",
      "Epoch 16/100\n",
      "\u001b[1m315/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321us/step - loss: 0.0030\n",
      "Epoch 16: loss improved from 0.00297 to 0.00294, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408us/step - loss: 0.0030 - val_loss: 0.0012\n",
      "Epoch 17/100\n",
      "\u001b[1m174/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289us/step - loss: 0.0029\n",
      "Epoch 17: loss improved from 0.00294 to 0.00284, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395us/step - loss: 0.0029 - val_loss: 0.0011\n",
      "Epoch 18/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0028\n",
      "Epoch 18: loss improved from 0.00284 to 0.00283, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332us/step - loss: 0.0028 - val_loss: 0.0011\n",
      "Epoch 19/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0028\n",
      "Epoch 19: loss improved from 0.00283 to 0.00273, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394us/step - loss: 0.0028 - val_loss: 9.8084e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m296/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 0.0027\n",
      "Epoch 20: loss improved from 0.00273 to 0.00269, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 426us/step - loss: 0.0027 - val_loss: 9.2555e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m201/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251us/step - loss: 0.0027\n",
      "Epoch 21: loss did not improve from 0.00269\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step - loss: 0.0027 - val_loss: 0.0013\n",
      "Epoch 22/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0026\n",
      "Epoch 22: loss improved from 0.00269 to 0.00264, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392us/step - loss: 0.0026 - val_loss: 9.6699e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m222/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227us/step - loss: 0.0025\n",
      "Epoch 23: loss improved from 0.00264 to 0.00261, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359us/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 24/100\n",
      "\u001b[1m352/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288us/step - loss: 0.0025\n",
      "Epoch 24: loss improved from 0.00261 to 0.00258, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395us/step - loss: 0.0025 - val_loss: 9.7302e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0026\n",
      "Epoch 25: loss did not improve from 0.00258\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - loss: 0.0026 - val_loss: 8.2154e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0026\n",
      "Epoch 26: loss improved from 0.00258 to 0.00258, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362us/step - loss: 0.0026 - val_loss: 9.7586e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0025\n",
      "Epoch 27: loss did not improve from 0.00258\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - loss: 0.0026 - val_loss: 8.4309e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m236/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 0.0025\n",
      "Epoch 28: loss did not improve from 0.00258\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 29/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - loss: 0.0026\n",
      "Epoch 29: loss improved from 0.00258 to 0.00257, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344us/step - loss: 0.0026 - val_loss: 9.7866e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m322/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313us/step - loss: 0.0026\n",
      "Epoch 30: loss did not improve from 0.00257\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 0.0026 - val_loss: 9.8822e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m279/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - loss: 0.0026\n",
      "Epoch 31: loss improved from 0.00257 to 0.00255, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437us/step - loss: 0.0026 - val_loss: 9.8699e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m203/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step - loss: 0.0026\n",
      "Epoch 32: loss improved from 0.00255 to 0.00250, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393us/step - loss: 0.0026 - val_loss: 8.1990e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - loss: 0.0027\n",
      "Epoch 33: loss did not improve from 0.00250\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - loss: 0.0026 - val_loss: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - loss: 0.0025\n",
      "Epoch 34: loss improved from 0.00250 to 0.00248, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342us/step - loss: 0.0025 - val_loss: 9.9072e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m262/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - loss: 0.0025\n",
      "Epoch 35: loss did not improve from 0.00248\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 562us/step - loss: 0.0025 - val_loss: 8.7981e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - loss: 0.0025\n",
      "Epoch 36: loss did not improve from 0.00248\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303us/step - loss: 0.0025 - val_loss: 7.6486e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0024\n",
      "Epoch 37: loss did not improve from 0.00248\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0024 - val_loss: 9.3704e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m214/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step - loss: 0.0025\n",
      "Epoch 38: loss did not improve from 0.00248\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319us/step - loss: 0.0025 - val_loss: 7.4955e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m333/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301us/step - loss: 0.0025\n",
      "Epoch 39: loss did not improve from 0.00248\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - loss: 0.0025 - val_loss: 6.6024e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237us/step - loss: 0.0025\n",
      "Epoch 40: loss improved from 0.00248 to 0.00244, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step - loss: 0.0025 - val_loss: 7.1106e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m212/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - loss: 0.0025\n",
      "Epoch 41: loss did not improve from 0.00244\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324us/step - loss: 0.0025 - val_loss: 8.0104e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m214/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235us/step - loss: 0.0024\n",
      "Epoch 42: loss did not improve from 0.00244\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296us/step - loss: 0.0024 - val_loss: 9.9363e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m330/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - loss: 0.0025\n",
      "Epoch 43: loss did not improve from 0.00244\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step - loss: 0.0025 - val_loss: 7.6311e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m203/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step - loss: 0.0024\n",
      "Epoch 44: loss did not improve from 0.00244\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - loss: 0.0024 - val_loss: 6.9160e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m181/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279us/step - loss: 0.0025\n",
      "Epoch 45: loss improved from 0.00244 to 0.00244, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391us/step - loss: 0.0025 - val_loss: 8.3723e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m316/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319us/step - loss: 0.0024\n",
      "Epoch 46: loss improved from 0.00244 to 0.00240, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 456us/step - loss: 0.0024 - val_loss: 6.5310e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m181/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279us/step - loss: 0.0025\n",
      "Epoch 47: loss did not improve from 0.00240\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326us/step - loss: 0.0025 - val_loss: 6.7363e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - loss: 0.0025\n",
      "Epoch 48: loss did not improve from 0.00240\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301us/step - loss: 0.0025 - val_loss: 6.4668e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m347/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 0.0025\n",
      "Epoch 49: loss did not improve from 0.00240\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 0.0025 - val_loss: 6.3811e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m202/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250us/step - loss: 0.0025\n",
      "Epoch 50: loss did not improve from 0.00240\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310us/step - loss: 0.0024 - val_loss: 7.5850e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0025\n",
      "Epoch 51: loss did not improve from 0.00240\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299us/step - loss: 0.0025 - val_loss: 6.6278e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m357/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281us/step - loss: 0.0024\n",
      "Epoch 52: loss did not improve from 0.00240\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 0.0024 - val_loss: 7.7586e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m337/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299us/step - loss: 0.0026\n",
      "Epoch 53: loss did not improve from 0.00240\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364us/step - loss: 0.0026 - val_loss: 6.9015e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m203/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step - loss: 0.0024\n",
      "Epoch 54: loss did not improve from 0.00240\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0024 - val_loss: 7.6898e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m188/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269us/step - loss: 0.0025\n",
      "Epoch 55: loss did not improve from 0.00240\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315us/step - loss: 0.0025 - val_loss: 6.7129e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m312/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323us/step - loss: 0.0025\n",
      "Epoch 56: loss did not improve from 0.00240\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step - loss: 0.0025 - val_loss: 7.2048e-04\n",
      "Epoch 56: early stopping\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "Epoch 1/100\n",
      "\u001b[1m293/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344us/step - loss: 0.0721 \n",
      "Epoch 1: loss improved from inf to 0.03204, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 565us/step - loss: 0.0650 - val_loss: 0.0062\n",
      "Epoch 2/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244us/step - loss: 0.0113\n",
      "Epoch 2: loss improved from 0.03204 to 0.00978, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400us/step - loss: 0.0108 - val_loss: 0.0039\n",
      "Epoch 3/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0081\n",
      "Epoch 3: loss improved from 0.00978 to 0.00723, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step - loss: 0.0078 - val_loss: 0.0030\n",
      "Epoch 4/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0062\n",
      "Epoch 4: loss improved from 0.00723 to 0.00558, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336us/step - loss: 0.0060 - val_loss: 0.0027\n",
      "Epoch 5/100\n",
      "\u001b[1m306/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329us/step - loss: 0.0045\n",
      "Epoch 5: loss improved from 0.00558 to 0.00441, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 443us/step - loss: 0.0045 - val_loss: 0.0024\n",
      "Epoch 6/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0040\n",
      "Epoch 6: loss improved from 0.00441 to 0.00382, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 0.0039 - val_loss: 0.0021\n",
      "Epoch 7/100\n",
      "\u001b[1m178/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283us/step - loss: 0.0035\n",
      "Epoch 7: loss improved from 0.00382 to 0.00340, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358us/step - loss: 0.0035 - val_loss: 0.0020\n",
      "Epoch 8/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0034\n",
      "Epoch 8: loss improved from 0.00340 to 0.00322, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335us/step - loss: 0.0033 - val_loss: 0.0019\n",
      "Epoch 9/100\n",
      "\u001b[1m275/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step - loss: 0.0030\n",
      "Epoch 9: loss improved from 0.00322 to 0.00303, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494us/step - loss: 0.0030 - val_loss: 0.0017\n",
      "Epoch 10/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - loss: 0.0030\n",
      "Epoch 10: loss improved from 0.00303 to 0.00290, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358us/step - loss: 0.0029 - val_loss: 0.0015\n",
      "Epoch 11/100\n",
      "\u001b[1m271/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 562us/step - loss: 0.0028\n",
      "Epoch 11: loss improved from 0.00290 to 0.00279, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 0.0028 - val_loss: 0.0012\n",
      "Epoch 12/100\n",
      "\u001b[1m323/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311us/step - loss: 0.0027\n",
      "Epoch 12: loss improved from 0.00279 to 0.00267, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 419us/step - loss: 0.0027 - val_loss: 0.0011\n",
      "Epoch 13/100\n",
      "\u001b[1m200/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252us/step - loss: 0.0028\n",
      "Epoch 13: loss improved from 0.00267 to 0.00260, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415us/step - loss: 0.0027 - val_loss: 0.0014\n",
      "Epoch 14/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0026\n",
      "Epoch 14: loss improved from 0.00260 to 0.00259, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396us/step - loss: 0.0026 - val_loss: 0.0011\n",
      "Epoch 15/100\n",
      "\u001b[1m204/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - loss: 0.0026\n",
      "Epoch 15: loss improved from 0.00259 to 0.00259, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step - loss: 0.0026 - val_loss: 0.0012\n",
      "Epoch 16/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - loss: 0.0025\n",
      "Epoch 16: loss improved from 0.00259 to 0.00253, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0025 - val_loss: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m301/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - loss: 0.0026\n",
      "Epoch 17: loss did not improve from 0.00253\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 0.0026 - val_loss: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m210/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step - loss: 0.0024\n",
      "Epoch 18: loss improved from 0.00253 to 0.00244, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - loss: 0.0024 - val_loss: 0.0011\n",
      "Epoch 19/100\n",
      "\u001b[1m194/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260us/step - loss: 0.0024\n",
      "Epoch 19: loss did not improve from 0.00244\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327us/step - loss: 0.0024 - val_loss: 7.5793e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m310/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328us/step - loss: 0.0025\n",
      "Epoch 20: loss improved from 0.00244 to 0.00242, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472us/step - loss: 0.0025 - val_loss: 7.3589e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0024\n",
      "Epoch 21: loss improved from 0.00242 to 0.00239, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 0.0024 - val_loss: 7.2464e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m185/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273us/step - loss: 0.0024\n",
      "Epoch 22: loss improved from 0.00239 to 0.00234, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401us/step - loss: 0.0024 - val_loss: 8.8291e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m268/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - loss: 0.0023\n",
      "Epoch 23: loss did not improve from 0.00234\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412us/step - loss: 0.0023 - val_loss: 9.1973e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0025\n",
      "Epoch 24: loss did not improve from 0.00234\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - loss: 0.0024 - val_loss: 6.6423e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0023\n",
      "Epoch 25: loss improved from 0.00234 to 0.00232, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372us/step - loss: 0.0023 - val_loss: 7.9752e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m200/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252us/step - loss: 0.0024\n",
      "Epoch 26: loss did not improve from 0.00232\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313us/step - loss: 0.0024 - val_loss: 8.6501e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m320/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - loss: 0.0024\n",
      "Epoch 27: loss did not improve from 0.00232\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - loss: 0.0024 - val_loss: 6.7755e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m172/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293us/step - loss: 0.0023\n",
      "Epoch 28: loss did not improve from 0.00232\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.0023 - val_loss: 8.7782e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m202/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250us/step - loss: 0.0023\n",
      "Epoch 29: loss improved from 0.00232 to 0.00229, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step - loss: 0.0023 - val_loss: 6.3251e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m311/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 0.0023\n",
      "Epoch 30: loss did not improve from 0.00229\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 0.0023 - val_loss: 7.3295e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m205/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0023\n",
      "Epoch 31: loss improved from 0.00229 to 0.00228, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step - loss: 0.0023 - val_loss: 8.8753e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m173/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291us/step - loss: 0.0024\n",
      "Epoch 32: loss did not improve from 0.00228\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331us/step - loss: 0.0023 - val_loss: 7.2659e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m275/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step - loss: 0.0023\n",
      "Epoch 33: loss did not improve from 0.00228\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412us/step - loss: 0.0023 - val_loss: 7.3954e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m346/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291us/step - loss: 0.0023\n",
      "Epoch 34: loss improved from 0.00228 to 0.00226, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397us/step - loss: 0.0023 - val_loss: 6.5994e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m320/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - loss: 0.0022\n",
      "Epoch 35: loss did not improve from 0.00226\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388us/step - loss: 0.0022 - val_loss: 6.6264e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m175/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288us/step - loss: 0.0023\n",
      "Epoch 36: loss improved from 0.00226 to 0.00225, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411us/step - loss: 0.0023 - val_loss: 7.8513e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m310/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490us/step - loss: 0.0023\n",
      "Epoch 37: loss did not improve from 0.00225\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 0.0023 - val_loss: 8.0092e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m283/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - loss: 0.0022\n",
      "Epoch 38: loss improved from 0.00225 to 0.00218, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 449us/step - loss: 0.0022 - val_loss: 7.9420e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0021\n",
      "Epoch 39: loss did not improve from 0.00218\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - loss: 0.0021 - val_loss: 6.0292e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m356/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283us/step - loss: 0.0022\n",
      "Epoch 40: loss improved from 0.00218 to 0.00217, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414us/step - loss: 0.0022 - val_loss: 6.8727e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m315/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319us/step - loss: 0.0022\n",
      "Epoch 41: loss did not improve from 0.00217\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 0.0022 - val_loss: 8.5223e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m201/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251us/step - loss: 0.0021\n",
      "Epoch 42: loss improved from 0.00217 to 0.00214, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405us/step - loss: 0.0021 - val_loss: 8.1818e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244us/step - loss: 0.0022\n",
      "Epoch 43: loss improved from 0.00214 to 0.00213, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - loss: 0.0022 - val_loss: 5.2848e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m323/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step - loss: 0.0022\n",
      "Epoch 44: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400us/step - loss: 0.0022 - val_loss: 7.1217e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m192/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262us/step - loss: 0.0021\n",
      "Epoch 45: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step - loss: 0.0021 - val_loss: 8.2108e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m200/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252us/step - loss: 0.0023\n",
      "Epoch 46: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313us/step - loss: 0.0023 - val_loss: 6.4680e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m272/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 371us/step - loss: 0.0021\n",
      "Epoch 47: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432us/step - loss: 0.0021 - val_loss: 7.1648e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244us/step - loss: 0.0020\n",
      "Epoch 48: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - loss: 0.0021 - val_loss: 5.3267e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m197/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256us/step - loss: 0.0022\n",
      "Epoch 49: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326us/step - loss: 0.0022 - val_loss: 6.9422e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m263/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - loss: 0.0021\n",
      "Epoch 50: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - loss: 0.0021 - val_loss: 8.1188e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m203/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248us/step - loss: 0.0022\n",
      "Epoch 51: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - loss: 0.0022 - val_loss: 6.2803e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m200/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252us/step - loss: 0.0021\n",
      "Epoch 52: loss did not improve from 0.00213\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310us/step - loss: 0.0021 - val_loss: 5.8880e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m177/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285us/step - loss: 0.0021\n",
      "Epoch 53: loss improved from 0.00213 to 0.00210, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367us/step - loss: 0.0021 - val_loss: 5.4967e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m327/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - loss: 0.0021\n",
      "Epoch 54: loss improved from 0.00210 to 0.00210, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - loss: 0.0021 - val_loss: 8.0804e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0021\n",
      "Epoch 55: loss did not improve from 0.00210\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - loss: 0.0021 - val_loss: 5.8389e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0021\n",
      "Epoch 56: loss did not improve from 0.00210\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303us/step - loss: 0.0021 - val_loss: 7.4493e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m327/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step - loss: 0.0021\n",
      "Epoch 57: loss did not improve from 0.00210\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - loss: 0.0021 - val_loss: 4.8760e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m214/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step - loss: 0.0022\n",
      "Epoch 58: loss did not improve from 0.00210\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299us/step - loss: 0.0022 - val_loss: 5.3666e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m209/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - loss: 0.0021\n",
      "Epoch 59: loss did not improve from 0.00210\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301us/step - loss: 0.0021 - val_loss: 5.1536e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0021\n",
      "Epoch 60: loss did not improve from 0.00210\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323us/step - loss: 0.0021 - val_loss: 5.4733e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m312/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323us/step - loss: 0.0021\n",
      "Epoch 61: loss did not improve from 0.00210\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - loss: 0.0021 - val_loss: 5.3927e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0022\n",
      "Epoch 62: loss did not improve from 0.00210\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - loss: 0.0021 - val_loss: 8.7691e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0022\n",
      "Epoch 63: loss did not improve from 0.00210\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331us/step - loss: 0.0022 - val_loss: 6.7783e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m284/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - loss: 0.0021\n",
      "Epoch 64: loss improved from 0.00210 to 0.00206, saving model to ../models/multivariate_load_foreacasting_load_cnn.keras\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472us/step - loss: 0.0021 - val_loss: 6.1321e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m211/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - loss: 0.0021\n",
      "Epoch 65: loss did not improve from 0.00206\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - loss: 0.0021 - val_loss: 7.8865e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m172/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293us/step - loss: 0.0022\n",
      "Epoch 66: loss did not improve from 0.00206\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333us/step - loss: 0.0022 - val_loss: 6.5201e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - loss: 0.0021\n",
      "Epoch 67: loss did not improve from 0.00206\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 0.0021 - val_loss: 8.4461e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m351/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287us/step - loss: 0.0021\n",
      "Epoch 68: loss did not improve from 0.00206\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391us/step - loss: 0.0021 - val_loss: 6.0949e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m191/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264us/step - loss: 0.0021\n",
      "Epoch 69: loss did not improve from 0.00206\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322us/step - loss: 0.0021 - val_loss: 7.3037e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0020\n",
      "Epoch 70: loss did not improve from 0.00206\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309us/step - loss: 0.0021 - val_loss: 9.5129e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m290/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.0021\n",
      "Epoch 71: loss did not improve from 0.00206\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.0021 - val_loss: 5.1199e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m207/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243us/step - loss: 0.0021\n",
      "Epoch 72: loss did not improve from 0.00206\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310us/step - loss: 0.0021 - val_loss: 5.1150e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m208/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - loss: 0.0021\n",
      "Epoch 73: loss did not improve from 0.00206\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - loss: 0.0021 - val_loss: 6.1239e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m206/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - loss: 0.0021\n",
      "Epoch 74: loss did not improve from 0.00206\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - loss: 0.0021 - val_loss: 6.1388e-04\n",
      "Epoch 74: early stopping\n",
      "Restoring model weights from the end of the best epoch: 64.\n",
      "Average Test Loss over 10 experiments: 0.001200236938893795\n",
      "Best Loss: 0.0010987152345478535\n",
      "Best model saved to ../models/multivariate_load_foreacasting_load_cnn.keras\n"
     ]
    }
   ],
   "source": [
    "#5. Build, train, and evaluate the model\n",
    "multivariate_load_foreacasting_load_temp_included_model_path_cnn = '../models/multivariate_load_foreacasting_load_cnn.keras'\n",
    "num_target_features = 1  # The number of output features for the model only load for now\n",
    "\n",
    "# Define the number of experiments\n",
    "num_experiments = 10\n",
    "\n",
    "# Initialize variables to accumulate the total loss and track the best model\n",
    "total_loss = 0\n",
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "for _ in range(num_experiments):\n",
    "    # Build and train the model\n",
    "    model, history = build_and_train_model(\n",
    "        xs_train_scaled, ys_train_scaled, model_config, num_target_features, path_to_save_model=multivariate_load_foreacasting_load_temp_included_model_path_cnn\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss = model.evaluate(xs_test_scaled, ys_test_scaled, verbose=0)\n",
    "    total_loss += loss\n",
    "    \n",
    "    # Update the best model if the current model's loss is lower than the best loss\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_model = model\n",
    "\n",
    "# Calculate the average loss\n",
    "average_loss = total_loss / num_experiments\n",
    "\n",
    "print(f'Average Test Loss over {num_experiments} experiments: {average_loss}')\n",
    "print(f'Best Loss: {best_loss}')\n",
    "\n",
    "# # Save the best model\n",
    "# best_model.save(multivariate_load_foreacasting_load_temp_included_model_path)\n",
    "print(f'Best model saved to {multivariate_load_foreacasting_load_temp_included_model_path_cnn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span> \n",
       "\n",
       " max_pooling1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " flatten_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)                     \u001b[38;5;34m608\u001b[0m \n",
       "\n",
       " max_pooling1d_9 (\u001b[38;5;33mMaxPooling1D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)                       \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dropout_9 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)                       \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " flatten_9 (\u001b[38;5;33mFlatten\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_9 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                          \u001b[38;5;34m33\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,925</span> (7.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,925\u001b[0m (7.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">641</span> (2.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m641\u001b[0m (2.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,284</span> (5.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,284\u001b[0m (5.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0010987152345478535\n"
     ]
    }
   ],
   "source": [
    "#     # Evaluate the model\n",
    "loss = model.evaluate(xs_test_scaled, ys_test_scaled, verbose=0)\n",
    "print(f'Test Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multivariate_load_foreacasting_load_temp_included_model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the best model from the path into model variable\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[43mmultivariate_load_foreacasting_load_temp_included_model_path\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest model loaded from path.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'multivariate_load_foreacasting_load_temp_included_model_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the best model from the path into model variable\n",
    "model = tf.keras.models.load_model(multivariate_load_foreacasting_load_temp_included_model_path_cnn)\n",
    "print('Best model loaded from path.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Build, train, and evaluate the model\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_scaled = model.predict(xs_test_scaled) # contains only load\n",
    "predictions_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale the predictions and actual values\n",
    "# predictions=> contains values for target column (Load)\n",
    "# but our scaler was trained on all columns so we have to inverse transform all columns\n",
    "# so we need to padd with zeros the other columns\n",
    "num_of_missing_training_features = train_data_df.shape[1] - num_target_features\n",
    "\n",
    "padding_for_missing_training_features = np.zeros((predictions_scaled.shape[0], num_of_missing_training_features))\n",
    "padding_for_missing_training_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_for_missing_training_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_be_invert_from_scaling = np.hstack([padding_for_missing_training_features, predictions_scaled])\n",
    "data_to_be_invert_from_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_be_invert_from_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model outputs \n",
    "predictions= scaler.inverse_transform(data_to_be_invert_from_scaling)[:, target_col]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_for_missing_training_features = np.zeros((ys_test_scaled.shape[0], num_of_missing_training_features))\n",
    "ys_test_scaled = np.hstack([padding_for_missing_training_features, ys_test_scaled])\n",
    "ys_test_scaled\n",
    "ys_test = scaler.inverse_transform(ys_test_scaled)[:,target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(abs(ys_test-predictions)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(ys_test, predictions)\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Plot the results\n",
    "test_index =test_data_df[seq_length:].index\n",
    "hours_to_plot = -1 # Approximately one month\n",
    "\n",
    "plot_results(test_index, ys_test, predictions, hours_to_plot=hours_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results_from_to(test_index, ys_test, predictions,'2019-06-01', '2019-07-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your test_index, ys_test_rescaled, and predictions_rescaled already defined\n",
    "plot_seasonal_comparison(test_index, ys_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your test_index, ys_test_rescaled, and predictions_rescaled already defined\n",
    "plot_monthly_comparison(test_index, ys_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your test_index, ys_test_rescaled, and predictions_rescaled already defined\n",
    "plot_weekday_comparison(test_index, ys_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hourly_comparison(test_index, ys_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col_name = train_data_df.columns[target_col]\n",
    "predictions_df = pd.DataFrame(predictions, columns=[target_col_name], index=test_index)\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "train_data_df['Load (kW)'].plot(ax=ax, label=\"Train\")\n",
    "test_data_df['Load (kW)'].plot(ax=ax, label=\"Test\")\n",
    "predictions_df.plot(ax=ax, label=\"Forecasted Load\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = test_data_df['Load (kW)']\n",
    "mape = np.mean(np.abs((actual[seq_length:] - predictions) / actual[seq_length:])) * 100\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
