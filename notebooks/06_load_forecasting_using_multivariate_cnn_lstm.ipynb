{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, LSTM\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seasonal_comparison(test_index, ys_test_rescaled, predictions_rescaled):\n",
    "    \"\"\"\n",
    "    Plot the average actual and forecasted load for each season.\n",
    "\n",
    "    Parameters:\n",
    "    - test_index: Datetime index for the test data.\n",
    "    - ys_test_rescaled: Rescaled actual test values.\n",
    "    - predictions_rescaled: Rescaled forecasted values.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame to hold the test and prediction data\n",
    "    data = pd.DataFrame({\n",
    "        'Actual': ys_test_rescaled.flatten(),\n",
    "        'Predicted': predictions_rescaled.flatten()\n",
    "    }, index=test_index)\n",
    "    \n",
    "    # Add a 'Season' column to the DataFrame\n",
    "    data['Month'] = data.index.month\n",
    "    data['Season'] = data['Month'].apply(lambda x: (\n",
    "        'Winter' if x in [12, 1, 2] else\n",
    "        'Spring' if x in [3, 4, 5] else\n",
    "        'Summer' if x in [6, 7, 8] else\n",
    "        'Autumn'\n",
    "    ))\n",
    "    \n",
    "    # Group by season and calculate the mean for actual and predicted values\n",
    "    seasonal_data = data.groupby('Season').mean()\n",
    "    \n",
    "    # Plot the seasonal comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(seasonal_data.index, seasonal_data['Actual'], label='Actual Load', marker='o')\n",
    "    plt.plot(seasonal_data.index, seasonal_data['Predicted'], label='Forecasted Load', marker='o')\n",
    "    plt.xlabel('Season')\n",
    "    plt.ylabel('Average Load')\n",
    "    plt.title('Average Actual Load and Forecasted Load by Season')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_monthly_comparison(test_index, ys_test_rescaled, predictions_rescaled):\n",
    "    \"\"\"\n",
    "    Plot the average actual and forecasted load by month.\n",
    "\n",
    "    Parameters:\n",
    "    - test_index: Datetime index for the test data.\n",
    "    - ys_test_rescaled: Rescaled actual test values.\n",
    "    - predictions_rescaled: Rescaled forecasted values.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame to hold the test and prediction data\n",
    "    data = pd.DataFrame({\n",
    "        'Actual': ys_test_rescaled.flatten(),\n",
    "        'Predicted': predictions_rescaled.flatten()\n",
    "    }, index=test_index)\n",
    "    \n",
    "    # Add a 'Month' column to the DataFrame\n",
    "    data['Month'] = data.index.month\n",
    "\n",
    "    # Group by month and calculate the mean for actual and predicted values\n",
    "    monthly_data = data.groupby('Month').mean()\n",
    "\n",
    "    # Create month labels corresponding to the months present in the dataset\n",
    "    month_labels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    available_months = monthly_data.index\n",
    "    available_labels = [month_labels[month - 1] for month in available_months]\n",
    "\n",
    "    # Plot the monthly comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(monthly_data.index, monthly_data['Actual'], label='Actual Load', marker='o')\n",
    "    plt.plot(monthly_data.index, monthly_data['Predicted'], label='Forecasted Load', marker='o')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Average Load')\n",
    "    plt.title('Average Actual Load and Forecasted Load by Month')\n",
    "    plt.xticks(ticks=monthly_data.index, labels=available_labels)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weekday_comparison(test_index, ys_test_rescaled, predictions_rescaled):\n",
    "    \"\"\"\n",
    "    Plot the average actual and forecasted load by day of the week.\n",
    "\n",
    "    Parameters:\n",
    "    - test_index: Datetime index for the test data.\n",
    "    - ys_test_rescaled: Rescaled actual test values.\n",
    "    - predictions_rescaled: Rescaled forecasted values.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame to hold the test and prediction data\n",
    "    data = pd.DataFrame({\n",
    "        'Actual': ys_test_rescaled.flatten(),\n",
    "        'Predicted': predictions_rescaled.flatten()\n",
    "    }, index=test_index)\n",
    "    \n",
    "    # Add a 'DayOfWeek' column to the DataFrame\n",
    "    data['DayOfWeek'] = data.index.dayofweek\n",
    "\n",
    "    # Group by day of the week and calculate the mean for actual and predicted values\n",
    "    weekday_data = data.groupby('DayOfWeek').mean()\n",
    "\n",
    "    # Create day labels corresponding to the days of the week\n",
    "    day_labels = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    available_days = weekday_data.index\n",
    "    available_day_labels = [day_labels[day] for day in available_days]\n",
    "\n",
    "    # Plot the weekday comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(weekday_data.index, weekday_data['Actual'], label='Actual Load', marker='o')\n",
    "    plt.plot(weekday_data.index, weekday_data['Predicted'], label='Forecasted Load', marker='o')\n",
    "    plt.xlabel('Day of the Week')\n",
    "    plt.ylabel('Average Load')\n",
    "    plt.title('Average Actual Load and Forecasted Load by Day of the Week')\n",
    "    plt.xticks(ticks=weekday_data.index, labels=available_day_labels)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hourly_comparison(test_index, ys_test_rescaled, predictions_rescaled):\n",
    "    \"\"\"\n",
    "    Plot the average actual and forecasted load by hour of the day.\n",
    "\n",
    "    Parameters:\n",
    "    - test_index: Datetime index for the test data.\n",
    "    - ys_test_rescaled: Rescaled actual test values.\n",
    "    - predictions_rescaled: Rescaled forecasted values.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame to hold the test and prediction data\n",
    "    data = pd.DataFrame({\n",
    "        'Actual': ys_test_rescaled.flatten(),\n",
    "        'Predicted': predictions_rescaled.flatten()\n",
    "    }, index=test_index)\n",
    "    \n",
    "    # Add an 'Hour' column to the DataFrame\n",
    "    data['Hour'] = data.index.hour\n",
    "\n",
    "    # Group by hour of the day and calculate the mean for actual and predicted values\n",
    "    hourly_data = data.groupby('Hour').mean()\n",
    "\n",
    "    # Create hour labels corresponding to the hours of the day\n",
    "    available_hours = hourly_data.index\n",
    "    available_hour_labels = [f'{hour}:00' for hour in available_hours]\n",
    "\n",
    "    # Plot the hourly comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(hourly_data.index, hourly_data['Actual'], label='Actual Load', marker='o')\n",
    "    plt.plot(hourly_data.index, hourly_data['Predicted'], label='Forecasted Load', marker='o')\n",
    "    plt.xlabel('Hour of the Day')\n",
    "    plt.ylabel('Average Load')\n",
    "    plt.title('Average Actual Load and Forecasted Load by Hour of the Day')\n",
    "    plt.xticks(ticks=hourly_data.index, labels=available_hour_labels)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_from_to(test_index, ys_test_rescaled, predictions_rescaled, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Plot the actual and forecasted load for a specified date range.\n",
    "\n",
    "    Parameters:\n",
    "    - test_index: Datetime index for the test data.\n",
    "    - ys_test_rescaled: Rescaled actual test values.\n",
    "    - predictions_rescaled: Rescaled forecasted values.\n",
    "    - start_date: Start date for the plot.\n",
    "    - end_date: End date for the plot.\n",
    "    \"\"\"\n",
    "    # Convert start_date and end_date to datetime if they are strings\n",
    "    if isinstance(start_date, str):\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "    if isinstance(end_date, str):\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    # Create a boolean mask for the date range\n",
    "    mask = (test_index >= start_date) & (test_index <= end_date)\n",
    "\n",
    "    # Apply the mask to the test data and predictions\n",
    "    time_index = test_index[mask]\n",
    "    ys_test_range = ys_test_rescaled[mask]\n",
    "    predictions_range = predictions_rescaled[mask]\n",
    "\n",
    "    # Plotting the actual and forecasted load for the specified date range\n",
    "    plt.figure(figsize=(24, 5))\n",
    "    plt.plot(time_index, ys_test_range.flatten(), label='Actual Load')\n",
    "    plt.plot(time_index, predictions_range.flatten(), label='Forecasted Load')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Load')\n",
    "    plt.title(f'Actual Load and Forecasted Load from {start_date:%Y-%m-%d} to {end_date:%Y-%m-%d}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Customize x-axis to show date and day of the week\n",
    "    plt.xticks(ticks=time_index[::24], labels=[f\"{date:%Y-%m-%d}\\n{date:%A}\" for date in time_index[::24]], rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_results(test_index, ys_test_rescaled, predictions_rescaled, hours_to_plot=720):\n",
    "    \"\"\"\n",
    "    Plot the actual and forecasted load for the first month.\n",
    "\n",
    "    Parameters:\n",
    "    - test_index: Datetime index for the test data.\n",
    "    - ys_test_rescaled: Rescaled actual test values.\n",
    "    - predictions_rescaled: Rescaled forecasted values.\n",
    "    - hours_in_month: Number of hours to plot for the first month.\n",
    "    \"\"\"\n",
    "    # Adjust the test index to include only the first month of data\n",
    "    time_index = test_index[:hours_to_plot]\n",
    "    ys_test_first_month = ys_test_rescaled[:hours_to_plot]\n",
    "    predictions_first_month = predictions_rescaled[:hours_to_plot]\n",
    "\n",
    "    # Plotting the initial month of actual and forecasted load\n",
    "    plt.figure(figsize=(24, 5))\n",
    "    plt.plot(time_index, ys_test_first_month.flatten(), label='Actual Load')\n",
    "    plt.plot(time_index, predictions_first_month.flatten(), label='Forecasted Load')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Load')\n",
    "    plt.title('Actual Load and Forecasted Load for the First Month')\n",
    "    plt.legend()\n",
    "\n",
    "    # Customize x-axis to show date and day of the week\n",
    "    plt.xticks(ticks=time_index[::24], labels=[f\"{date:%Y-%m-%d}\\n{date:%A}\" for date in time_index[::24]], rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_model(xs_train, ys_train, model_config, num_target_features, path_to_save_model):\n",
    "    \"\"\"\n",
    "    Build, train, and evaluate an LSTM model.\n",
    "\n",
    "    Parameters:\n",
    "    - xs_train, ys_train: Training data.\n",
    "    - model_config: Dictionary containing LSTM layers configuration and other model parameters.\n",
    "    - num_target_features: Number of output features for the model.\n",
    "    - path_to_save_model: Path to save the trained model.\n",
    "\n",
    "    Returns:\n",
    "    - model: Trained LSTM model.\n",
    "    - history: Training history of the model.\n",
    "    \"\"\"\n",
    "    # Determine the split index for training and validation sets\n",
    "    split_index = int(len(xs_train) * 0.8)  # 80% for training, 20% for validation\n",
    "\n",
    "    # Split the data sequentially\n",
    "    xs_train_split = xs_train[:split_index]\n",
    "    ys_train_split = ys_train[:split_index]\n",
    "    xs_val_split = xs_train[split_index:]\n",
    "    ys_val_split = ys_train[split_index:]\n",
    "\n",
    "    num_time_steps = xs_train.shape[1]\n",
    "    num_features = xs_train.shape[2]\n",
    "    input_shape = (num_time_steps, num_features)\n",
    "\n",
    "    # Initialize the Sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Conv1D layers based on model_config\n",
    "    for layer_config in model_config['cnn_layers']:\n",
    "        model.add(Conv1D(filters=layer_config.get('filters', 32), kernel_size=layer_config.get('kernel_size', 5), activation='relu', input_shape=input_shape))\n",
    "        model.add(MaxPooling1D(pool_size=layer_config.get('pool_size', 2)))\n",
    "        \n",
    "        # Reset input_shape after the first layer\n",
    "        input_shape = None\n",
    "\n",
    "        if 'dropout' in layer_config:\n",
    "            model.add(Dropout(layer_config['dropout']))\n",
    "\n",
    "    # Add LSTM layers based on model_config\n",
    "    for layer_config in model_config['lstm_layers']:\n",
    "        model.add(LSTM(units=layer_config.get('units', 50), activation='tanh', return_sequences=layer_config.get('return_sequences', False)))\n",
    "\n",
    "        if 'dropout' in layer_config:\n",
    "            model.add(Dropout(layer_config['dropout']))\n",
    "\n",
    "    # Add a Flatten layer if needed (after Conv1D and LSTM layers)\n",
    "    #model.add(Flatten())\n",
    "\n",
    "    # Add the output layer\n",
    "    model.add(Dense(num_target_features))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Define callbacks: EarlyStopping and ModelCheckpoint\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50, verbose=1, restore_best_weights=True)\n",
    "    checkpoint = ModelCheckpoint(path_to_save_model, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(xs_train_split, ys_train_split, \n",
    "                        epochs=model_config.get('epochs', 50), \n",
    "                        batch_size=model_config.get('batch_size', 32), \n",
    "                        validation_data=(xs_val_split, ys_val_split), \n",
    "                        callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length, forecast_horizon, target_col):\n",
    "    target_col_index = target_col\n",
    "    xs, ys = [], []\n",
    "    target_col_name = data.columns[target_col_index]  \n",
    "    for i in range(len(data) - seq_length - forecast_horizon + 1):\n",
    "        x = data.iloc[i:(i + seq_length)].values\n",
    "      #  x = data.iloc[i:(i + seq_length)].drop(columns=[target_col_name]).values \n",
    "        y = data.iloc[(i + seq_length):(i + seq_length + forecast_horizon), target_col].values\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "#data_df = pd.read_csv('../../data/processed/actuals_data.csv', parse_dates=['Time'], index_col='Time')\n",
    "# Dataset with actual weather variables\n",
    "# data_df = pd.read_csv('../data/interim/precovid-data/train/load_with_actual_weather_variables_dataset.csv', parse_dates=['Time'], index_col='Time')\n",
    "\n",
    "# Dataset without COVID-19 with forecasted weather variables\n",
    "#datapath= '../data/processed/covid-data/covid_dataset_without_outliers.csv'\n",
    "#datapath= '../data/processed/covid-data/covid_dataset_without_outliers.csv'\n",
    "train_data_df = pd.read_csv('../data/processed/covid-data/train/covid_dataset_actuals_train.csv', parse_dates=['Time'], index_col='Time')\n",
    "# Corrected test data: \n",
    "test_data_df = pd.read_csv('../data/processed/covid-data/test/covid_dataset_corrected_weather_forecasts.csv', parse_dates=['Time'], index_col='Time')\n",
    "#Forecasts: \n",
    "#test_data_df = pd.read_csv('../data/processed/covid-data/test/covid_dataset_forecasts_test.csv', parse_dates=['Time'], index_col='Time')\n",
    "#Actuals:\n",
    "#test_data_df = pd.read_csv('../data/processed/covid-data/test/covid_dataset_actuals_test.csv', parse_dates=['Time'], index_col='Time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit number of data to have same size as evalution period\n",
    "EVALUATION_PERIOD_LENGTH = 744\n",
    "test_data_df = test_data_df.iloc[:EVALUATION_PERIOD_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_col = train_data_df.pop('Load (kW)')\n",
    "train_data_df['Load (kW)'] = load_col\n",
    "\n",
    "target_col = (train_data_df.columns.get_loc('Load (kW)'))\n",
    "num_target_features = 1\n",
    "scaler_num_features = train_data_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Scale data\n",
    "scaler = MinMaxScaler()\n",
    "train_data_df_scaled = pd.DataFrame(scaler.fit_transform(train_data_df), columns=train_data_df.columns, index=train_data_df.index)\n",
    "test_data_df_scaled = pd.DataFrame(scaler.fit_transform(test_data_df), columns=test_data_df.columns, index=test_data_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scaler to a file using joblib\n",
    "#dump(scaler, '../models/scalers/scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#2. Create sequences pairs of input and output\n",
    "#In this case we have to configure the target_col-1 to be the index of the target column in the data_df in order to assign in the ys variable\n",
    "# and have input output pairs of sequences\n",
    "seq_length = 4\n",
    "forecast_horizon = 1\n",
    "xs_train_scaled, ys_train_scaled = create_sequences(train_data_df_scaled, seq_length, forecast_horizon, target_col)\n",
    "xs_test_scaled, ys_test_scaled = create_sequences(test_data_df_scaled, seq_length, forecast_horizon, target_col)\n",
    "\n",
    "print(xs_train_scaled.shape, ys_train_scaled.shape)\n",
    "print(xs_test_scaled.shape, ys_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 4))\n",
    "train_data_df['Load (kW)'].plot(ax=ax, label=\"Train\")\n",
    "test_data_df['Load (kW)'].plot(ax=ax, label=\"Test\", linestyle='dotted')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Define the model configuration\n",
    "model_config = {\n",
    "  'cnn_layers': [\n",
    "        {'filters': 32, 'kernel_size': 3, 'activation': 'relu', 'pool_size': 2, 'dropout': 0.2}, #ÎŸutput shape: (None, 3, 64)\n",
    "    ],\n",
    "    'lstm_layers': [\n",
    "        {'units': 16, 'return_sequences': True,'dropout': 0.2},\n",
    "        {'units': 8, 'return_sequences': False}\n",
    "    ],\n",
    "    'epochs': 60,\n",
    "    'batch_size': 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_train_scaled.shape\n",
    "print('Num Samples:', xs_train_scaled.shape[0])\n",
    "print('Num Time Steps:', xs_train_scaled.shape[1])\n",
    "print('Num Features:', xs_train_scaled.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Build, train, and evaluate the model\n",
    "multivariate_load_foreacasting_stlf_path_cnn_lstm_model_path = '../models/multivariate_load_foreacasting_stlf_path_cnn_lstm_model.keras'\n",
    "num_target_features = 1  # The number of output features for the model only load for now\n",
    "\n",
    "# Define the number of experiments\n",
    "num_experiments = 10\n",
    "\n",
    "# Initialize variables to accumulate the total loss and track the best model\n",
    "total_loss = 0\n",
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "for _ in range(num_experiments):\n",
    "    # Build and train the model\n",
    "  \n",
    "    model, history = build_and_train_model(\n",
    "            xs_train_scaled, ys_train_scaled, model_config, num_target_features, path_to_save_model=multivariate_load_foreacasting_stlf_path_cnn_lstm_model_path\n",
    "        )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss = model.evaluate(xs_test_scaled, ys_test_scaled, verbose=0)\n",
    "    total_loss += loss\n",
    "    \n",
    "    # Update the best model if the current model's loss is lower than the best loss\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_model = model\n",
    "\n",
    "# Calculate the average loss\n",
    "average_loss = total_loss / num_experiments\n",
    "\n",
    "print(f'Average Test Loss over {num_experiments} experiments: {average_loss}')\n",
    "print(f'Best Loss: {best_loss}')\n",
    "\n",
    "# Save the best model\n",
    "best_model.save(multivariate_load_foreacasting_stlf_path_cnn_lstm_model_path)\n",
    "print(f'Best model saved to {multivariate_load_foreacasting_stlf_path_cnn_lstm_model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # Evaluate the model\n",
    "loss = model.evaluate(xs_test_scaled, ys_test_scaled, verbose=0)\n",
    "print(f'Test Loss: {loss}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from the path into model variable\n",
    "model = tf.keras.models.load_model(multivariate_load_foreacasting_stlf_path_cnn_lstm_model_path)\n",
    "print('Best model loaded from path.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Build, train, and evaluate the model\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_scaled = model.predict(xs_test_scaled) # contains only load\n",
    "predictions_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale the predictions and actual values\n",
    "# predictions=> contains values for target column (Load)\n",
    "# but our scaler was trained on all columns so we have to inverse transform all columns\n",
    "# so we need to padd with zeros the other columns\n",
    "num_of_missing_training_features = train_data_df.shape[1] - num_target_features\n",
    "\n",
    "padding_for_missing_training_features = np.zeros((predictions_scaled.shape[0], num_of_missing_training_features))\n",
    "padding_for_missing_training_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_for_missing_training_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_be_invert_from_scaling = np.hstack([padding_for_missing_training_features, predictions_scaled])\n",
    "data_to_be_invert_from_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_be_invert_from_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model outputs \n",
    "predictions= scaler.inverse_transform(data_to_be_invert_from_scaling)[:, target_col]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_for_missing_training_features = np.zeros((ys_test_scaled.shape[0], num_of_missing_training_features))\n",
    "ys_test_scaled = np.hstack([padding_for_missing_training_features, ys_test_scaled])\n",
    "ys_test_scaled\n",
    "ys_test = scaler.inverse_transform(ys_test_scaled)[:,target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(abs(ys_test-predictions)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(ys_test, predictions)\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Plot the results\n",
    "test_index =test_data_df[seq_length:].index\n",
    "hours_to_plot = -1 # Approximately one month\n",
    "\n",
    "plot_results(test_index, ys_test, predictions, hours_to_plot=hours_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results_from_to(test_index, ys_test, predictions,'2019-06-01', '2019-07-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your test_index, ys_test_rescaled, and predictions_rescaled already defined\n",
    "plot_seasonal_comparison(test_index, ys_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your test_index, ys_test_rescaled, and predictions_rescaled already defined\n",
    "plot_monthly_comparison(test_index, ys_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your test_index, ys_test_rescaled, and predictions_rescaled already defined\n",
    "plot_weekday_comparison(test_index, ys_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hourly_comparison(test_index, ys_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col_name = train_data_df.columns[target_col]\n",
    "predictions_df = pd.DataFrame(predictions, columns=[target_col_name], index=test_index)\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "train_data_df['Load (kW)'].plot(ax=ax, label=\"Train\")\n",
    "test_data_df['Load (kW)'].plot(ax=ax, label=\"Test\")\n",
    "predictions_df.plot(ax=ax, label=\"Forecasted Load\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = test_data_df['Load (kW)']\n",
    "mape = np.mean(np.abs((actual[seq_length:] - predictions) / actual[seq_length:])) * 100\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
